{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main.ipynb","provenance":[],"mount_file_id":"1Cj8SmZGPW_xUDfhBP6AvYYRVsAWqCd-x","authorship_tag":"ABX9TyOVXkoaXH1o8Na8NRmi6eaP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"fpDg5q7TKkqA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594326468189,"user_tz":240,"elapsed":2430,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"a04c5d7b-7461-4793-a031-7fe3a97f14ac"},"source":["# Import all the necessary libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datetime import datetime as dt\n","import itertools\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sc4ArwOHv68J","colab":{}},"source":["model_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Summer Research/Data/model_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fbvQp0Hwloe","colab_type":"code","colab":{}},"source":["x = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","x = x.iloc[:, 1:]\n","y = model_data.A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TU0HJPK92o4X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"executionInfo":{"status":"ok","timestamp":1594303433509,"user_tz":240,"elapsed":347,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"74122918-120f-4719-9142-122968bec0b6"},"source":["x.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FTAG</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>HST</th>\n","      <th>HF</th>\n","      <th>AF</th>\n","      <th>AC</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>B365A</th>\n","      <th>BWH</th>\n","      <th>BWD</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>LBH</th>\n","      <th>LBD</th>\n","      <th>LBA</th>\n","      <th>SBH</th>\n","      <th>SBD</th>\n","      <th>WHH</th>\n","      <th>WHD</th>\n","      <th>WHA</th>\n","      <th>SJD</th>\n","      <th>SJA</th>\n","      <th>VCD</th>\n","      <th>VCA</th>\n","      <th>Bb1X2</th>\n","      <th>BbMxH</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbAvA</th>\n","      <th>BbOU</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbAv&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAH</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>BbMxAHA</th>\n","      <th>BbAvAHA</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>14.0</td>\n","      <td>16.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>3.00</td>\n","      <td>2.10</td>\n","      <td>3.25</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.10</td>\n","      <td>2.1</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.2</td>\n","      <td>2.20</td>\n","      <td>3.2</td>\n","      <td>2.80</td>\n","      <td>3.25</td>\n","      <td>3.40</td>\n","      <td>3.25</td>\n","      <td>3.10</td>\n","      <td>56.0</td>\n","      <td>2.40</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>3.05</td>\n","      <td>36.0</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>22.0</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>1.92</td>\n","      <td>1.84</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>15.0</td>\n","      <td>14.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>1.72</td>\n","      <td>4.35</td>\n","      <td>3.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>1.83</td>\n","      <td>3.8</td>\n","      <td>3.75</td>\n","      <td>3.20</td>\n","      <td>1.83</td>\n","      <td>4.33</td>\n","      <td>3.6</td>\n","      <td>4.33</td>\n","      <td>3.2</td>\n","      <td>1.72</td>\n","      <td>3.25</td>\n","      <td>1.83</td>\n","      <td>3.30</td>\n","      <td>1.80</td>\n","      <td>56.0</td>\n","      <td>5.65</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>23.0</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1.93</td>\n","      <td>1.86</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7.0</td>\n","      <td>12.0</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.87</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>2.95</td>\n","      <td>2.2</td>\n","      <td>2.25</td>\n","      <td>3.00</td>\n","      <td>2.88</td>\n","      <td>2.30</td>\n","      <td>3.2</td>\n","      <td>2.30</td>\n","      <td>3.2</td>\n","      <td>2.62</td>\n","      <td>3.20</td>\n","      <td>3.00</td>\n","      <td>3.25</td>\n","      <td>2.80</td>\n","      <td>56.0</td>\n","      <td>2.60</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.87</td>\n","      <td>36.0</td>\n","      <td>2.24</td>\n","      <td>2.04</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>21.0</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>2.11</td>\n","      <td>2.05</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>3.45</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>4.75</td>\n","      <td>1.7</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>4.50</td>\n","      <td>1.70</td>\n","      <td>3.4</td>\n","      <td>1.70</td>\n","      <td>3.3</td>\n","      <td>4.33</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>55.0</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>4.79</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.94</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>23.0</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1.83</td>\n","      <td>1.76</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.40</td>\n","      <td>2.90</td>\n","      <td>3.35</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>2.45</td>\n","      <td>2.5</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.70</td>\n","      <td>3.2</td>\n","      <td>2.75</td>\n","      <td>3.1</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.38</td>\n","      <td>3.25</td>\n","      <td>2.35</td>\n","      <td>56.0</td>\n","      <td>3.30</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.35</td>\n","      <td>36.0</td>\n","      <td>2.23</td>\n","      <td>2.02</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>21.0</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>2.04</td>\n","      <td>2.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   FTAG  HTHG  HTAG  D  E  F  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0   2.0   2.0   2.0  0  0  1  ...  22.0  -0.25     2.10     2.01     1.92     1.84\n","1   2.0   0.0   1.0  0  1  0  ...  23.0   0.75     2.05     2.00     1.93     1.86\n","2   0.0   0.0   0.0  0  0  1  ...  21.0   0.00     1.85     1.81     2.11     2.05\n","3   0.0   0.0   0.0  0  0  1  ...  23.0  -0.75     2.19     2.10     1.83     1.76\n","4   0.0   0.0   0.0  0  0  1  ...  21.0   0.25     1.89     1.86     2.04     2.00\n","\n","[5 rows x 51 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"z6hc7RPTwrzj","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt \n","plt.rc(\"font\", size=14)\n","import seaborn as sns\n","sns.set(style=\"white\")\n","sns.set(style=\"whitegrid\", color_codes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PX1Puakpw1eb","colab_type":"code","colab":{}},"source":["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)\n","logistic_regression = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logistic_regression.fit(x_train,y_train)\n","y_pred = logistic_regression.predict(x_test)\n","y_train_predict = logistic_regression.predict(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBlewAR6xDQa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593627773863,"user_tz":240,"elapsed":790,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"8de1e00d-a740-4200-aa4e-fa2dc0185915"},"source":["accuracy = metrics.accuracy_score(y_test, y_pred)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentageaccuracy = metrics.accuracy_score(y_test, y_pred)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentage"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["81.0704960835509"]},"metadata":{"tags":[]},"execution_count":142}]},{"cell_type":"code","metadata":{"id":"_yjhrm4tmFQT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593627776712,"user_tz":240,"elapsed":808,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"464b3db5-2a87-4034-ae04-0946b99de086"},"source":["accuracy = metrics.accuracy_score(y_train, y_train_predict)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentageaccuracy = metrics.accuracy_score(y_train, y_train_predict)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentage"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["82.85714285714286"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"code","metadata":{"id":"3cVTgruDxmTL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593627795432,"user_tz":240,"elapsed":1607,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"aca4e9d1-4995-49a8-c905-113b277e10a2"},"source":["x = x.astype(float) \n","y = y.astype(float) \n","import statsmodels.api as sm\n","logit_model=sm.Logit(y,x)\n","result=logit_model.fit()\n","print(result.summary2())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.347597\n","         Iterations 8\n","                         Results: Logit\n","================================================================\n","Model:              Logit            Pseudo R-squared: 0.498    \n","Dependent Variable: A                AIC:              1875.4387\n","Date:               2020-07-01 18:23 BIC:              2173.4949\n","No. Observations:   2551             Log-Likelihood:   -886.72  \n","Df Model:           50               LL-Null:          -1765.0  \n","Df Residuals:       2500             LLR p-value:      0.0000   \n","Converged:          1.0000           Scale:            1.0000   \n","No. Iterations:     8.0000                                      \n","-----------------------------------------------------------------\n","            Coef.   Std.Err.     z      P>|z|    [0.025    0.975]\n","-----------------------------------------------------------------\n","FTAG       -2.0191    0.1156  -17.4624  0.0000   -2.2457  -1.7925\n","HTHG        1.9886    0.3089    6.4388  0.0000    1.3833   2.5940\n","HTAG        0.0948    0.3267    0.2901  0.7717   -0.5456   0.7352\n","D          -5.1703    5.6619   -0.9132  0.3611  -16.2674   5.9268\n","E          -5.4655    5.6828   -0.9618  0.3362  -16.6036   5.6726\n","F          -5.6923    5.6593   -1.0058  0.3145  -16.7843   5.3998\n","HST         0.1594    0.0207    7.6859  0.0000    0.1188   0.2001\n","HF         -0.0007    0.0168   -0.0436  0.9652   -0.0336   0.0321\n","AF          0.0021    0.0162    0.1282  0.8980   -0.0297   0.0338\n","AC          0.0449    0.0234    1.9214  0.0547   -0.0009   0.0906\n","HR         -0.6798    0.2683   -2.5336  0.0113   -1.2057  -0.1539\n","AR          0.5311    0.1871    2.8381  0.0045    0.1643   0.8978\n","B365H       0.4059    0.6096    0.6659  0.5055   -0.7889   1.6008\n","B365D      -0.1335    0.5591   -0.2388  0.8112   -1.2293   0.9623\n","B365A       0.0080    0.1517    0.0530  0.9577   -0.2894   0.3055\n","BWH         0.5596    0.4942    1.1322  0.2576   -0.4091   1.5283\n","BWD         0.0245    0.4463    0.0549  0.9563   -0.8502   0.8992\n","BWA         0.2494    0.1632    1.5283  0.1264   -0.0704   0.5692\n","GBH        -0.3000    0.5902   -0.5084  0.6112   -1.4567   0.8567\n","GBA        -0.1666    0.1603   -1.0393  0.2987   -0.4807   0.1476\n","IWH        -0.2217    0.4745   -0.4672  0.6404   -1.1516   0.7083\n","LBH         0.8007    0.3567    2.2448  0.0248    0.1016   1.4997\n","LBD        -0.1550    0.4140   -0.3744  0.7081   -0.9664   0.6564\n","LBA         0.1155    0.1398    0.8261  0.4088   -0.1585   0.3894\n","SBH        -0.0780    0.5410   -0.1441  0.8854   -1.1383   0.9824\n","SBD         0.3324    0.6182    0.5378  0.5907   -0.8792   1.5441\n","WHH        -0.1592    0.4308   -0.3695  0.7117   -1.0036   0.6852\n","WHD        -0.3748    0.3903   -0.9604  0.3369   -1.1397   0.3901\n","WHA         0.1568    0.1298    1.2080  0.2270   -0.0976   0.4111\n","SJD        -0.6195    0.5305   -1.1678  0.2429   -1.6594   0.4203\n","SJA         0.0891    0.1556    0.5724  0.5671   -0.2160   0.3941\n","VCD        -0.0277    0.4753   -0.0583  0.9535   -0.9592   0.9038\n","VCA        -0.3124    0.1431   -2.1828  0.0290   -0.5930  -0.0319\n","Bb1X2       0.0035    0.0125    0.2828  0.7773   -0.0210   0.0281\n","BbMxH       0.0833    0.6054    0.1376  0.8905   -1.1032   1.2698\n","BbAvH      -1.0912    1.3861   -0.7873  0.4311   -3.8078   1.6254\n","BbMxD       0.7500    0.8142    0.9211  0.3570   -0.8458   2.3458\n","BbAvD      -1.0403    1.7412   -0.5975  0.5502   -4.4530   2.3724\n","BbMxA       0.1145    0.1729    0.6619  0.5080   -0.2245   0.4534\n","BbAvA       0.1348    0.5055    0.2667  0.7897   -0.8559   1.1255\n","BbOU        0.0067    0.0213    0.3163  0.7518   -0.0350   0.0485\n","BbMx>2.5    2.1221    1.6809    1.2625  0.2068   -1.1723   5.4166\n","BbAv>2.5   -0.4013    2.5467   -0.1576  0.8748   -5.3927   4.5900\n","BbMx<2.5    0.6306    2.2387    0.2817  0.7782   -3.7572   5.0184\n","BbAv<2.5    1.5959    2.9147    0.5475  0.5840   -4.1168   7.3085\n","BbAH        0.0050    0.0191    0.2648  0.7912   -0.0323   0.0424\n","BbAHh       0.4000    0.6650    0.6015  0.5475   -0.9034   1.7034\n","BbMxAHH     1.4266    0.9322    1.5303  0.1259   -0.4005   3.2538\n","BbAvAHH    -1.7543    1.2333   -1.4225  0.1549   -4.1714   0.6629\n","BbMxAHA    -0.0181    0.1580   -0.1143  0.9090   -0.3277   0.2916\n","BbAvAHA    -0.1748    0.2662   -0.6567  0.5114   -0.6965   0.3469\n","================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PhOgUfLcprse","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593440702761,"user_tz":240,"elapsed":619,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"91d5962a-43d3-46a0-d33a-42e78b0734ba"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.82      0.82       383\n","           1       0.82      0.83      0.82       383\n","\n","    accuracy                           0.82       766\n","   macro avg       0.82      0.82      0.82       766\n","weighted avg       0.82      0.82      0.82       766\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1UMbU54wptWE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1593440708716,"user_tz":240,"elapsed":1283,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"8707c1b5-80d8-4b3d-a652-1b4058010ef7"},"source":["from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","logit_roc_auc = roc_auc_score(y_test, logistic_regression.predict(x_test))\n","fpr, tpr, thresholds = roc_curve(y_test, logistic_regression.predict_proba(x_test)[:,1])\n","plt.figure()\n","plt.plot(fpr, tpr, label='Logistic Regression for Home Wins (area = %0.2f)' % logit_roc_auc)\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic')\n","plt.legend(loc=\"lower right\")\n","#plt.savefig('Log_ROC')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FDQLtqMThzAh","colab":{}},"source":["x1 = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","x1= x1.iloc[:, 1:]\n","y1 = model_data.B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqbuKfx5XKBJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1593630279039,"user_tz":240,"elapsed":828,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"dd07ddc3-75e1-4818-9092-4a2b1f4c513e"},"source":["x1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FTAG</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>HST</th>\n","      <th>HF</th>\n","      <th>AF</th>\n","      <th>AC</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>B365A</th>\n","      <th>BWH</th>\n","      <th>BWD</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>LBH</th>\n","      <th>LBD</th>\n","      <th>LBA</th>\n","      <th>SBH</th>\n","      <th>SBD</th>\n","      <th>WHH</th>\n","      <th>WHD</th>\n","      <th>WHA</th>\n","      <th>SJD</th>\n","      <th>SJA</th>\n","      <th>VCD</th>\n","      <th>VCA</th>\n","      <th>Bb1X2</th>\n","      <th>BbMxH</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbAvA</th>\n","      <th>BbOU</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbAv&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAH</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>BbMxAHA</th>\n","      <th>BbAvAHA</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>14.0</td>\n","      <td>16.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>3.00</td>\n","      <td>2.10</td>\n","      <td>3.25</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.200</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>3.25</td>\n","      <td>3.40</td>\n","      <td>3.25</td>\n","      <td>3.10</td>\n","      <td>56.0</td>\n","      <td>2.40</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>3.05</td>\n","      <td>36.0</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>22.0</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>1.92</td>\n","      <td>1.84</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>15.0</td>\n","      <td>14.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>1.72</td>\n","      <td>4.35</td>\n","      <td>3.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>1.83</td>\n","      <td>3.80</td>\n","      <td>3.75</td>\n","      <td>3.20</td>\n","      <td>1.83</td>\n","      <td>4.33</td>\n","      <td>3.600</td>\n","      <td>4.33</td>\n","      <td>3.20</td>\n","      <td>1.72</td>\n","      <td>3.25</td>\n","      <td>1.83</td>\n","      <td>3.30</td>\n","      <td>1.80</td>\n","      <td>56.0</td>\n","      <td>5.65</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>23.0</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1.93</td>\n","      <td>1.86</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7.0</td>\n","      <td>12.0</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.87</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>2.95</td>\n","      <td>2.20</td>\n","      <td>2.25</td>\n","      <td>3.00</td>\n","      <td>2.88</td>\n","      <td>2.30</td>\n","      <td>3.200</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.62</td>\n","      <td>3.20</td>\n","      <td>3.00</td>\n","      <td>3.25</td>\n","      <td>2.80</td>\n","      <td>56.0</td>\n","      <td>2.60</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.87</td>\n","      <td>36.0</td>\n","      <td>2.24</td>\n","      <td>2.04</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>21.0</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>2.11</td>\n","      <td>2.05</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>3.45</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>4.75</td>\n","      <td>1.70</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>4.50</td>\n","      <td>1.70</td>\n","      <td>3.400</td>\n","      <td>1.70</td>\n","      <td>3.30</td>\n","      <td>4.33</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>55.0</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>4.79</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.94</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>23.0</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1.83</td>\n","      <td>1.76</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.40</td>\n","      <td>2.90</td>\n","      <td>3.35</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>2.45</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.70</td>\n","      <td>3.200</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.38</td>\n","      <td>3.25</td>\n","      <td>2.35</td>\n","      <td>56.0</td>\n","      <td>3.30</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.35</td>\n","      <td>36.0</td>\n","      <td>2.23</td>\n","      <td>2.02</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>21.0</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>2.04</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2546</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.44</td>\n","      <td>6.75</td>\n","      <td>4.60</td>\n","      <td>1.42</td>\n","      <td>7.00</td>\n","      <td>1.40</td>\n","      <td>6.00</td>\n","      <td>7.50</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.600</td>\n","      <td>7.00</td>\n","      <td>4.33</td>\n","      <td>1.44</td>\n","      <td>4.80</td>\n","      <td>1.40</td>\n","      <td>4.80</td>\n","      <td>1.44</td>\n","      <td>39.0</td>\n","      <td>8.30</td>\n","      <td>7.23</td>\n","      <td>4.90</td>\n","      <td>4.51</td>\n","      <td>1.47</td>\n","      <td>1.43</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.60</td>\n","      <td>2.43</td>\n","      <td>2.27</td>\n","      <td>20.0</td>\n","      <td>1.25</td>\n","      <td>1.95</td>\n","      <td>1.89</td>\n","      <td>2.03</td>\n","      <td>1.97</td>\n","    </tr>\n","    <tr>\n","      <th>2547</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.40</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.30</td>\n","      <td>3.60</td>\n","      <td>2.05</td>\n","      <td>3.25</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.40</td>\n","      <td>3.40</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.500</td>\n","      <td>3.40</td>\n","      <td>3.30</td>\n","      <td>2.15</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.60</td>\n","      <td>2.10</td>\n","      <td>39.0</td>\n","      <td>3.66</td>\n","      <td>3.38</td>\n","      <td>3.72</td>\n","      <td>3.45</td>\n","      <td>2.20</td>\n","      <td>2.10</td>\n","      <td>33.0</td>\n","      <td>1.77</td>\n","      <td>1.71</td>\n","      <td>2.24</td>\n","      <td>2.11</td>\n","      <td>20.0</td>\n","      <td>0.25</td>\n","      <td>2.13</td>\n","      <td>2.06</td>\n","      <td>1.84</td>\n","      <td>1.81</td>\n","    </tr>\n","    <tr>\n","      <th>2548</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>8.50</td>\n","      <td>1.40</td>\n","      <td>4.40</td>\n","      <td>7.75</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>1.37</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>4.333</td>\n","      <td>1.40</td>\n","      <td>4.20</td>\n","      <td>9.00</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>5.00</td>\n","      <td>9.00</td>\n","      <td>39.0</td>\n","      <td>1.41</td>\n","      <td>1.40</td>\n","      <td>5.07</td>\n","      <td>4.55</td>\n","      <td>9.10</td>\n","      <td>8.04</td>\n","      <td>32.0</td>\n","      <td>1.68</td>\n","      <td>1.63</td>\n","      <td>2.45</td>\n","      <td>2.23</td>\n","      <td>18.0</td>\n","      <td>-1.25</td>\n","      <td>1.93</td>\n","      <td>1.90</td>\n","      <td>2.02</td>\n","      <td>1.97</td>\n","    </tr>\n","    <tr>\n","      <th>2549</th>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>10.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.62</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>5.40</td>\n","      <td>4.50</td>\n","      <td>3.75</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>3.750</td>\n","      <td>5.00</td>\n","      <td>3.60</td>\n","      <td>1.70</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>4.20</td>\n","      <td>1.67</td>\n","      <td>39.0</td>\n","      <td>5.60</td>\n","      <td>5.02</td>\n","      <td>4.28</td>\n","      <td>3.93</td>\n","      <td>1.70</td>\n","      <td>1.65</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.58</td>\n","      <td>2.50</td>\n","      <td>2.31</td>\n","      <td>18.0</td>\n","      <td>0.75</td>\n","      <td>2.14</td>\n","      <td>2.08</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10.0</td>\n","      <td>15.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>1.60</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>5.75</td>\n","      <td>1.60</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>6.00</td>\n","      <td>1.55</td>\n","      <td>4.000</td>\n","      <td>1.62</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>4.30</td>\n","      <td>6.00</td>\n","      <td>39.0</td>\n","      <td>1.62</td>\n","      <td>1.57</td>\n","      <td>4.35</td>\n","      <td>3.95</td>\n","      <td>6.40</td>\n","      <td>5.80</td>\n","      <td>28.0</td>\n","      <td>1.67</td>\n","      <td>1.59</td>\n","      <td>2.43</td>\n","      <td>2.26</td>\n","      <td>22.0</td>\n","      <td>-1.00</td>\n","      <td>2.01</td>\n","      <td>1.96</td>\n","      <td>1.97</td>\n","      <td>1.90</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2551 rows × 51 columns</p>\n","</div>"],"text/plain":["      FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 51 columns]"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"code","metadata":{"id":"h-0mbCAJhxaQ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt \n","plt.rc(\"font\", size=14)\n","import seaborn as sns\n","sns.set(style=\"white\")\n","sns.set(style=\"whitegrid\", color_codes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Z-uqccPh9Bs","colab_type":"code","colab":{}},"source":["x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.3, random_state=4)\n","logistic_regression1 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logistic_regression1.fit(x1_train,y1_train)\n","y1_pred = logistic_regression1.predict(x1_test)\n","y1_train_predict = logistic_regression1.predict(x1_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ruOjvcMiUnT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593627906323,"user_tz":240,"elapsed":1103,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"ef1eacb8-c62c-4aee-99e9-bbbff479c3ba"},"source":["accuracy = metrics.accuracy_score(y1_test, y1_pred)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentage"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["87.0757180156658"]},"metadata":{"tags":[]},"execution_count":148}]},{"cell_type":"code","metadata":{"id":"6xp0Mk58m-SX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593627913067,"user_tz":240,"elapsed":952,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"05cd05f8-99b4-4eb1-d537-9ffa91081efc"},"source":["accuracy = metrics.accuracy_score(y1_train, y1_train_predict)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentage"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["89.29971988795519"]},"metadata":{"tags":[]},"execution_count":149}]},{"cell_type":"code","metadata":{"id":"KiU8eaVJiYvW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593458549322,"user_tz":240,"elapsed":1546,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"c9e0051d-fc7c-477d-9ada-8487d851fa37"},"source":["x1 = x1.astype(float) \n","y1 = y1.astype(float) \n","import statsmodels.api as sm\n","logit_model=sm.Logit(y1,x1)\n","result=logit_model.fit()\n","print(result.summary2())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.225297\n","         Iterations 9\n","                         Results: Logit\n","================================================================\n","Model:              Logit            Pseudo R-squared: 0.615    \n","Dependent Variable: B                AIC:              1259.4666\n","Date:               2020-06-29 19:22 BIC:              1580.8998\n","No. Observations:   2551             Log-Likelihood:   -574.73  \n","Df Model:           54               LL-Null:          -1494.1  \n","Df Residuals:       2496             LLR p-value:      0.0000   \n","Converged:          1.0000           Scale:            1.0000   \n","No. Iterations:     9.0000                                      \n","-----------------------------------------------------------------\n","            Coef.   Std.Err.     z      P>|z|    [0.025    0.975]\n","-----------------------------------------------------------------\n","FTAG        2.6151    0.1433   18.2458  0.0000    2.3342   2.8961\n","HTHG       -2.7487    0.2282  -12.0475  0.0000   -3.1958  -2.3015\n","HTAG        0.1144    0.1559    0.7335  0.4633   -0.1913   0.4200\n","D          -0.0449    0.3467   -0.1295  0.8970   -0.7244   0.6346\n","HS          0.0713    0.0300    2.3777  0.0174    0.0125   0.1300\n","AS         -0.0529    0.0349   -1.5157  0.1296   -0.1214   0.0155\n","HST        -0.1965    0.0433   -4.5369  0.0000   -0.2814  -0.1116\n","AST         0.0627    0.0486    1.2900  0.1970   -0.0326   0.1580\n","HF          0.0327    0.0212    1.5391  0.1238   -0.0089   0.0743\n","AF         -0.0231    0.0208   -1.1118  0.2662   -0.0639   0.0176\n","AC         -0.0362    0.0312   -1.1596  0.2462   -0.0973   0.0250\n","HR          0.6131    0.2816    2.1770  0.0295    0.0611   1.1652\n","AR         -0.7166    0.2902   -2.4692  0.0135   -1.2854  -0.1478\n","B365H       0.0073    0.4671    0.0156  0.9875   -0.9083   0.9229\n","B365D       0.4747    0.9515    0.4988  0.6179   -1.3903   2.3397\n","B365A      -0.0588    0.3263   -0.1804  0.8569   -0.6983   0.5806\n","BWH        -0.9345    0.4725   -1.9777  0.0480   -1.8607  -0.0084\n","BWD        -0.0849    0.7375   -0.1152  0.9083   -1.5304   1.3605\n","BWA         0.3972    0.3165    1.2548  0.2095   -0.2232   1.0176\n","GBH         0.6275    0.5621    1.1163  0.2643   -0.4743   1.7293\n","GBD        -0.5100    1.0583   -0.4819  0.6298   -2.5843   1.5642\n","GBA        -0.1488    0.3098   -0.4802  0.6310   -0.7561   0.4585\n","IWH         0.6239    0.4494    1.3882  0.1651   -0.2569   1.5046\n","IWD        -1.5452    0.8123   -1.9023  0.0571   -3.1372   0.0468\n","LBH        -0.1535    0.3508   -0.4378  0.6616   -0.8410   0.5339\n","LBD         0.4503    0.6711    0.6711  0.5022   -0.8649   1.7656\n","LBA         0.0756    0.2772    0.2729  0.7850   -0.4676   0.6188\n","SBH        -0.5992    0.5176   -1.1576  0.2470   -1.6138   0.4154\n","SBD         1.5177    0.9603    1.5805  0.1140   -0.3644   3.3998\n","SBA        -0.3499    0.3812   -0.9178  0.3587   -1.0971   0.3973\n","WHH         0.7245    0.3912    1.8518  0.0640   -0.0423   1.4914\n","WHD         0.6393    0.6006    1.0644  0.2872   -0.5379   1.8164\n","WHA         0.0937    0.2618    0.3577  0.7206   -0.4195   0.6068\n","SJH         0.2515    0.4379    0.5742  0.5658   -0.6069   1.1098\n","SJD         0.2024    0.8156    0.2482  0.8040   -1.3960   1.8009\n","SJA         0.1378    0.3178    0.4337  0.6645   -0.4851   0.7608\n","VCD         1.0406    0.7650    1.3603  0.1737   -0.4587   2.5400\n","Bb1X2       0.0317    0.0168    1.8836  0.0596   -0.0013   0.0647\n","BbMxH       0.6755    0.5418    1.2467  0.2125   -0.3865   1.7375\n","BbAvH      -0.9713    1.5162   -0.6406  0.5218   -3.9430   2.0003\n","BbMxD       0.3926    1.2287    0.3195  0.7493   -2.0156   2.8008\n","BbAvD      -1.7019    3.0936   -0.5501  0.5822   -7.7653   4.3615\n","BbMxA       0.3844    0.3636    1.0573  0.2904   -0.3282   1.0969\n","BbAvA      -1.4882    0.9986   -1.4904  0.1361   -3.4453   0.4689\n","BbOU       -0.0610    0.0277   -2.2067  0.0273   -0.1152  -0.0068\n","BbMx>2.5    1.1836    1.9029    0.6220  0.5339   -2.5460   4.9133\n","BbAv>2.5   -2.3981    2.1909   -1.0946  0.2737   -6.6922   1.8960\n","BbMx<2.5   -5.0526    2.9010   -1.7416  0.0816  -10.7385   0.6334\n","BbAv<2.5    4.5257    3.3551    1.3489  0.1774   -2.0502  11.1017\n","BbAH        0.0274    0.0257    1.0633  0.2876   -0.0231   0.0778\n","BbAHh      -2.1217    1.0148   -2.0907  0.0366   -4.1106  -0.1327\n","BbMxAHH     0.4044    1.1273    0.3587  0.7198   -1.8051   2.6139\n","BbAvAHH    -1.0712    1.4881   -0.7198  0.4716   -3.9879   1.8455\n","BbMxAHA    -0.4412    0.9061   -0.4869  0.6263   -2.2170   1.3347\n","BbAvAHA     1.0913    1.1759    0.9280  0.3534   -1.2135   3.3960\n","================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mQj9-MAOpOQr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593458556176,"user_tz":240,"elapsed":599,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"a450bf70-c87e-45be-d0ae-c1041138f0bc"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y1_test, y1_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.87      0.91       564\n","           1       0.70      0.89      0.78       202\n","\n","    accuracy                           0.87       766\n","   macro avg       0.83      0.88      0.85       766\n","weighted avg       0.89      0.87      0.88       766\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmM74jRBpIiU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1593458559050,"user_tz":240,"elapsed":1056,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"5803ebb5-5757-4a45-a284-efb1948650c4"},"source":["from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","logit_roc_auc = roc_auc_score(y1_test, logistic_regression1.predict(x1_test))\n","fpr, tpr, thresholds = roc_curve(y1_test, logistic_regression1.predict_proba(x1_test)[:,1])\n","plt.figure()\n","plt.plot(fpr, tpr, label='Logistic Regression for Away Wins (area = %0.2f)' % logit_roc_auc)\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic')\n","plt.legend(loc=\"lower right\")\n","#plt.savefig('Log_ROC')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"fcIDDn2EjRBj","colab_type":"code","colab":{}},"source":["x2 = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","x2 = x2.iloc[:, 1:]\n","y2 = model_data.C"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGPzcV8mXCTn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"ok","timestamp":1593623562207,"user_tz":240,"elapsed":679,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"76926f3f-7789-44e8-ceef-266b5909c919"},"source":["x2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>FTAG</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>HS</th>\n","      <th>AS</th>\n","      <th>HST</th>\n","      <th>AST</th>\n","      <th>HF</th>\n","      <th>AF</th>\n","      <th>AC</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>B365A</th>\n","      <th>BWH</th>\n","      <th>BWD</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBD</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>IWD</th>\n","      <th>LBH</th>\n","      <th>LBD</th>\n","      <th>LBA</th>\n","      <th>SBH</th>\n","      <th>SBD</th>\n","      <th>SBA</th>\n","      <th>WHH</th>\n","      <th>WHD</th>\n","      <th>WHA</th>\n","      <th>SJH</th>\n","      <th>SJD</th>\n","      <th>SJA</th>\n","      <th>VCD</th>\n","      <th>Bb1X2</th>\n","      <th>BbMxH</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbAvA</th>\n","      <th>BbOU</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbAv&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAH</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>BbMxAHA</th>\n","      <th>BbAvAHA</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>14.0</td>\n","      <td>16.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>3.00</td>\n","      <td>2.10</td>\n","      <td>3.25</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.20</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>3.0</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.200</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.40</td>\n","      <td>3.25</td>\n","      <td>56.0</td>\n","      <td>2.40</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>3.05</td>\n","      <td>36.0</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>22.0</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>1.92</td>\n","      <td>1.84</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>10.0</td>\n","      <td>12.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>15.0</td>\n","      <td>14.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>1.72</td>\n","      <td>4.35</td>\n","      <td>3.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>3.40</td>\n","      <td>1.83</td>\n","      <td>3.80</td>\n","      <td>3.1</td>\n","      <td>3.75</td>\n","      <td>3.20</td>\n","      <td>1.83</td>\n","      <td>4.33</td>\n","      <td>3.600</td>\n","      <td>1.75</td>\n","      <td>4.33</td>\n","      <td>3.20</td>\n","      <td>1.72</td>\n","      <td>4.00</td>\n","      <td>3.25</td>\n","      <td>1.83</td>\n","      <td>3.30</td>\n","      <td>56.0</td>\n","      <td>5.65</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>23.0</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1.93</td>\n","      <td>1.86</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>15.0</td>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>4.0</td>\n","      <td>12.0</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.87</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>3.30</td>\n","      <td>2.95</td>\n","      <td>2.20</td>\n","      <td>3.0</td>\n","      <td>2.25</td>\n","      <td>3.00</td>\n","      <td>2.88</td>\n","      <td>2.30</td>\n","      <td>3.200</td>\n","      <td>3.00</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.62</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>3.00</td>\n","      <td>3.25</td>\n","      <td>56.0</td>\n","      <td>2.60</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.87</td>\n","      <td>36.0</td>\n","      <td>2.24</td>\n","      <td>2.04</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>21.0</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>2.11</td>\n","      <td>2.05</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>15.0</td>\n","      <td>13.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>3.45</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>3.45</td>\n","      <td>4.75</td>\n","      <td>1.70</td>\n","      <td>3.2</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>4.50</td>\n","      <td>1.70</td>\n","      <td>3.400</td>\n","      <td>5.00</td>\n","      <td>1.70</td>\n","      <td>3.30</td>\n","      <td>4.33</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>3.25</td>\n","      <td>55.0</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>4.79</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.94</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>23.0</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1.83</td>\n","      <td>1.76</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.40</td>\n","      <td>2.90</td>\n","      <td>3.35</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>3.30</td>\n","      <td>2.45</td>\n","      <td>2.50</td>\n","      <td>3.0</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.70</td>\n","      <td>3.200</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.75</td>\n","      <td>3.20</td>\n","      <td>2.38</td>\n","      <td>3.25</td>\n","      <td>56.0</td>\n","      <td>3.30</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.35</td>\n","      <td>36.0</td>\n","      <td>2.23</td>\n","      <td>2.02</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>21.0</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>2.04</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2546</th>\n","      <td>375</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>6.0</td>\n","      <td>17.0</td>\n","      <td>5.0</td>\n","      <td>9.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.44</td>\n","      <td>6.75</td>\n","      <td>4.60</td>\n","      <td>1.42</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>6.00</td>\n","      <td>4.0</td>\n","      <td>7.50</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.600</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.33</td>\n","      <td>1.44</td>\n","      <td>7.50</td>\n","      <td>4.80</td>\n","      <td>1.40</td>\n","      <td>4.80</td>\n","      <td>39.0</td>\n","      <td>8.30</td>\n","      <td>7.23</td>\n","      <td>4.90</td>\n","      <td>4.51</td>\n","      <td>1.47</td>\n","      <td>1.43</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.60</td>\n","      <td>2.43</td>\n","      <td>2.27</td>\n","      <td>20.0</td>\n","      <td>1.25</td>\n","      <td>1.95</td>\n","      <td>1.89</td>\n","      <td>2.03</td>\n","      <td>1.97</td>\n","    </tr>\n","    <tr>\n","      <th>2547</th>\n","      <td>376</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>13.0</td>\n","      <td>14.0</td>\n","      <td>8.0</td>\n","      <td>9.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.40</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.30</td>\n","      <td>3.60</td>\n","      <td>2.05</td>\n","      <td>3.25</td>\n","      <td>3.30</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.2</td>\n","      <td>3.40</td>\n","      <td>3.40</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.500</td>\n","      <td>2.10</td>\n","      <td>3.40</td>\n","      <td>3.30</td>\n","      <td>2.15</td>\n","      <td>3.25</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.60</td>\n","      <td>39.0</td>\n","      <td>3.66</td>\n","      <td>3.38</td>\n","      <td>3.72</td>\n","      <td>3.45</td>\n","      <td>2.20</td>\n","      <td>2.10</td>\n","      <td>33.0</td>\n","      <td>1.77</td>\n","      <td>1.71</td>\n","      <td>2.24</td>\n","      <td>2.11</td>\n","      <td>20.0</td>\n","      <td>0.25</td>\n","      <td>2.13</td>\n","      <td>2.06</td>\n","      <td>1.84</td>\n","      <td>1.81</td>\n","    </tr>\n","    <tr>\n","      <th>2548</th>\n","      <td>377</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>7.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>8.50</td>\n","      <td>1.40</td>\n","      <td>4.40</td>\n","      <td>7.75</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.00</td>\n","      <td>1.37</td>\n","      <td>4.4</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>4.333</td>\n","      <td>8.00</td>\n","      <td>1.40</td>\n","      <td>4.20</td>\n","      <td>9.00</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>5.00</td>\n","      <td>39.0</td>\n","      <td>1.41</td>\n","      <td>1.40</td>\n","      <td>5.07</td>\n","      <td>4.55</td>\n","      <td>9.10</td>\n","      <td>8.04</td>\n","      <td>32.0</td>\n","      <td>1.68</td>\n","      <td>1.63</td>\n","      <td>2.45</td>\n","      <td>2.23</td>\n","      <td>18.0</td>\n","      <td>-1.25</td>\n","      <td>1.93</td>\n","      <td>1.90</td>\n","      <td>2.02</td>\n","      <td>1.97</td>\n","    </tr>\n","    <tr>\n","      <th>2549</th>\n","      <td>378</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>12.0</td>\n","      <td>12.0</td>\n","      <td>8.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>10.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.62</td>\n","      <td>5.00</td>\n","      <td>3.75</td>\n","      <td>1.65</td>\n","      <td>5.40</td>\n","      <td>3.6</td>\n","      <td>4.50</td>\n","      <td>3.75</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>3.750</td>\n","      <td>1.65</td>\n","      <td>5.00</td>\n","      <td>3.60</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>4.20</td>\n","      <td>39.0</td>\n","      <td>5.60</td>\n","      <td>5.02</td>\n","      <td>4.28</td>\n","      <td>3.93</td>\n","      <td>1.70</td>\n","      <td>1.65</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.58</td>\n","      <td>2.50</td>\n","      <td>2.31</td>\n","      <td>18.0</td>\n","      <td>0.75</td>\n","      <td>2.14</td>\n","      <td>2.08</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>379</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>14.0</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>7.0</td>\n","      <td>15.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>1.60</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>5.75</td>\n","      <td>1.60</td>\n","      <td>3.6</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>6.00</td>\n","      <td>1.55</td>\n","      <td>4.000</td>\n","      <td>5.60</td>\n","      <td>1.62</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>4.30</td>\n","      <td>39.0</td>\n","      <td>1.62</td>\n","      <td>1.57</td>\n","      <td>4.35</td>\n","      <td>3.95</td>\n","      <td>6.40</td>\n","      <td>5.80</td>\n","      <td>28.0</td>\n","      <td>1.67</td>\n","      <td>1.59</td>\n","      <td>2.43</td>\n","      <td>2.26</td>\n","      <td>22.0</td>\n","      <td>-1.00</td>\n","      <td>2.01</td>\n","      <td>1.96</td>\n","      <td>1.97</td>\n","      <td>1.90</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2551 rows × 56 columns</p>\n","</div>"],"text/plain":["      Unnamed: 0  FTAG  HTHG  HTAG  ...  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0              0   2.0   2.0   2.0  ...     2.10     2.01     1.92     1.84\n","1              1   2.0   0.0   1.0  ...     2.05     2.00     1.93     1.86\n","2              2   0.0   0.0   0.0  ...     1.85     1.81     2.11     2.05\n","3              3   0.0   0.0   0.0  ...     2.19     2.10     1.83     1.76\n","4              4   0.0   0.0   0.0  ...     1.89     1.86     2.04     2.00\n","...          ...   ...   ...   ...  ...      ...      ...      ...      ...\n","2546         375   1.0   0.0   1.0  ...     1.95     1.89     2.03     1.97\n","2547         376   0.0   0.0   0.0  ...     2.13     2.06     1.84     1.81\n","2548         377   0.0   1.0   0.0  ...     1.93     1.90     2.02     1.97\n","2549         378   3.0   2.0   2.0  ...     2.14     2.08     1.85     1.81\n","2550         379   2.0   2.0   1.0  ...     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 56 columns]"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"OG8oGIZxjacl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt \n","plt.rc(\"font\", size=14)\n","import seaborn as sns\n","sns.set(style=\"white\")\n","sns.set(style=\"whitegrid\", color_codes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HC7rq4SLjeEl","colab_type":"code","colab":{}},"source":["x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.3, random_state=4)\n","logistic_regression2 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logistic_regression2.fit(x2_train,y2_train)\n","y2_pred = logistic_regression2.predict(x2_test)\n","y2_train_predict = logistic_regression2.predict(x2_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4Kqcf74jyMX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593628106332,"user_tz":240,"elapsed":683,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"0006539c-3336-47bd-bbed-a6e73b2a6df9"},"source":["accuracy = metrics.accuracy_score(y2_test, y2_pred)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentage"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65.0130548302872"]},"metadata":{"tags":[]},"execution_count":156}]},{"cell_type":"code","metadata":{"id":"imoAUys7AK5u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593628128406,"user_tz":240,"elapsed":649,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"2fdf328d-f9eb-4649-f73f-ca3939cbb7fc"},"source":["accuracy = metrics.accuracy_score(y2_train, y2_train_predict)\n","accuracy_percentage = 100 * accuracy\n","accuracy_percentage"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63.69747899159663"]},"metadata":{"tags":[]},"execution_count":157}]},{"cell_type":"code","metadata":{"id":"vNDMT8aqksBR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593458888762,"user_tz":240,"elapsed":467,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"8b3eee3e-752e-4bd2-c43c-1a75cbc53de4"},"source":["x2 = x2.astype(float) \n","y2 = y2.astype(float) \n","import statsmodels.api as sm\n","logit_model=sm.Logit(y2,x2)\n","result=logit_model.fit()\n","print(result.summary2())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.511093\n","         Iterations 7\n","                         Results: Logit\n","=================================================================\n","Model:              Logit            Pseudo R-squared: 0.096     \n","Dependent Variable: C                AIC:              2717.5946 \n","Date:               2020-06-29 19:28 BIC:              3039.0279 \n","No. Observations:   2551             Log-Likelihood:   -1303.8   \n","Df Model:           54               LL-Null:          -1442.4   \n","Df Residuals:       2496             LLR p-value:      9.2436e-32\n","Converged:          1.0000           Scale:            1.0000    \n","No. Iterations:     7.0000                                       \n","------------------------------------------------------------------\n","               Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n","------------------------------------------------------------------\n","FTAG          -0.1583    0.0665  -2.3817  0.0172  -0.2887  -0.0280\n","HTHG           0.1066    0.1085   0.9826  0.3258  -0.1061   0.3194\n","HTAG          -0.3613    0.1092  -3.3082  0.0009  -0.5753  -0.1472\n","D             -1.5418    0.1952  -7.8986  0.0000  -1.9244  -1.1592\n","HS             0.0378    0.0186   2.0279  0.0426   0.0013   0.0744\n","AS             0.0385    0.0215   1.7945  0.0727  -0.0036   0.0806\n","HST           -0.1111    0.0266  -4.1729  0.0000  -0.1632  -0.0589\n","AST           -0.0642    0.0311  -2.0602  0.0394  -0.1252  -0.0031\n","HF             0.0055    0.0137   0.4009  0.6885  -0.0214   0.0324\n","AF             0.0143    0.0130   1.1056  0.2689  -0.0111   0.0398\n","AC             0.0006    0.0199   0.0309  0.9754  -0.0383   0.0396\n","HR             0.0249    0.1920   0.1296  0.8968  -0.3514   0.4012\n","AR            -0.0707    0.1605  -0.4409  0.6593  -0.3853   0.2438\n","B365H         -0.3847    0.3473  -1.1076  0.2680  -1.0654   0.2960\n","B365D         -0.0322    0.4735  -0.0680  0.9458  -0.9603   0.8959\n","B365A          0.0293    0.1228   0.2382  0.8117  -0.2114   0.2699\n","BWH            0.4112    0.3316   1.2403  0.2149  -0.2386   1.0611\n","BWD            0.2647    0.3879   0.6825  0.4949  -0.4955   1.0249\n","BWA           -0.2319    0.1379  -1.6811  0.0927  -0.5022   0.0385\n","GBH           -0.2198    0.3980  -0.5522  0.5808  -0.9999   0.5603\n","GBD           -0.5322    0.5204  -1.0227  0.3065  -1.5521   0.4878\n","GBA            0.1086    0.1411   0.7698  0.4414  -0.1679   0.3852\n","IWH           -0.3669    0.3093  -1.1863  0.2355  -0.9731   0.2393\n","IWD           -0.0205    0.4331  -0.0474  0.9622  -0.8694   0.8284\n","LBH           -0.2516    0.2375  -1.0591  0.2896  -0.7171   0.2140\n","LBD           -0.0224    0.3317  -0.0675  0.9462  -0.6724   0.6277\n","LBA           -0.0298    0.1203  -0.2474  0.8046  -0.2655   0.2060\n","SBH            0.2760    0.3589   0.7691  0.4418  -0.4273   0.9794\n","SBD           -0.7461    0.5215  -1.4308  0.1525  -1.7682   0.2759\n","SBA            0.2529    0.1463   1.7287  0.0839  -0.0338   0.5396\n","WHH           -0.2304    0.2771  -0.8312  0.4059  -0.7736   0.3128\n","WHD            0.1112    0.3214   0.3460  0.7294  -0.5187   0.7411\n","WHA           -0.1417    0.1104  -1.2836  0.1993  -0.3580   0.0746\n","SJH           -0.1314    0.2899  -0.4531  0.6504  -0.6996   0.4369\n","SJD            0.2251    0.4208   0.5350  0.5926  -0.5996   1.0498\n","SJA           -0.0430    0.1281  -0.3358  0.7370  -0.2940   0.2080\n","VCD           -0.0904    0.3713  -0.2434  0.8077  -0.8181   0.6373\n","Bb1X2         -0.0197    0.0109  -1.8027  0.0714  -0.0411   0.0017\n","BbMxH         -0.4018    0.3968  -1.0125  0.3113  -1.1795   0.3759\n","BbAvH          1.1910    0.9847   1.2095  0.2265  -0.7390   3.1211\n","BbMxD         -0.8423    0.6119  -1.3764  0.1687  -2.0416   0.3571\n","BbAvD          1.6329    1.4804   1.1030  0.2700  -1.2686   4.5344\n","BbMxA         -0.1913    0.1474  -1.2975  0.1944  -0.4802   0.0977\n","BbAvA          0.2955    0.4021   0.7348  0.4625  -0.4927   1.0837\n","BbOU           0.0067    0.0174   0.3885  0.6977  -0.0273   0.0408\n","BbMx>2.5      -1.8416    1.5344  -1.2002  0.2301  -4.8490   1.1658\n","BbAv>2.5       2.0549    1.7217   1.1935  0.2327  -1.3197   5.4294\n","BbMx<2.5       0.7321    1.3832   0.5293  0.5966  -1.9790   3.4431\n","BbAv<2.5      -0.6749    1.6148  -0.4180  0.6760  -3.8398   2.4900\n","BbAH           0.0019    0.0152   0.1227  0.9023  -0.0280   0.0317\n","BbAHh          0.1249    0.4930   0.2534  0.8000  -0.8413   1.0911\n","BbMxAHH       -0.4228    0.7046  -0.6001  0.5484  -1.8037   0.9581\n","BbAvAHH        0.6616    0.9002   0.7350  0.4624  -1.1027   2.4260\n","BbMxAHA       -0.0276    0.1907  -0.1446  0.8850  -0.4013   0.3461\n","BbAvAHA        0.0624    0.2740   0.2279  0.8197  -0.4746   0.5995\n","=================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3Nq8wAHzoAUU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593458901714,"user_tz":240,"elapsed":590,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"82940692-2c5f-4a4c-abd0-ed4591b44efb"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y2_test, y2_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.84      0.65      0.73       585\n","           1       0.35      0.61      0.44       181\n","\n","    accuracy                           0.64       766\n","   macro avg       0.59      0.63      0.59       766\n","weighted avg       0.72      0.64      0.66       766\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gclMuA44oCr-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1593458910371,"user_tz":240,"elapsed":683,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"122854b1-8e94-4ca8-bbbb-842025529365"},"source":["from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","logit_roc_auc = roc_auc_score(y2_test, logistic_regression2.predict(x2_test))\n","fpr, tpr, thresholds = roc_curve(y2_test, logistic_regression2.predict_proba(x2_test)[:,1])\n","plt.figure()\n","plt.plot(fpr, tpr, label='Logistic Regression for Draw  (area = %0.2f)' % logit_roc_auc)\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic')\n","plt.legend(loc=\"lower right\")\n","#plt.savefig('Log_ROC')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"d-7PmTar2KtZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593614872683,"user_tz":240,"elapsed":700,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"955a13c4-c627-48e4-924a-489442547eee"},"source":["print(x.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['Unnamed: 0', 'FTAG', 'HTHG', 'HTAG', 'D', 'E', 'F', 'HS', 'AS', 'HST',\n","       'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR', 'B365H', 'B365D',\n","       'B365A', 'BWH', 'BWD', 'BWA', 'GBH', 'GBD', 'GBA', 'IWH', 'IWD', 'IWA',\n","       'LBH', 'LBD', 'LBA', 'SBH', 'SBD', 'SBA', 'WHH', 'WHD', 'WHA', 'SJH',\n","       'SJD', 'SJA', 'VCH', 'VCD', 'VCA', 'Bb1X2', 'BbMxH', 'BbAvH', 'BbMxD',\n","       'BbAvD', 'BbMxA', 'BbAvA', 'BbOU', 'BbMx>2.5', 'BbAv>2.5', 'BbMx<2.5',\n","       'BbAv<2.5', 'BbAH', 'BbAHh', 'BbMxAHH', 'BbAvAHH', 'BbMxAHA',\n","       'BbAvAHA'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"edkgYH3Q2NIj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1594305450596,"user_tz":240,"elapsed":6096,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"55c21f9a-cec3-4e5a-d4a3-219797a5c858"},"source":["x = model_data.drop(['A', 'B', 'C', 'FTHG'],axis = 1)\n","y = model_data.A\n","\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","rfe = RFE(logreg, 10, step=1)\n","rfe = rfe.fit(x, y)\n","print(rfe.support_)\n","print(rfe.ranking_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[False  True  True False False False False False False False False False\n"," False False False False False  True  True False False False False False\n"," False  True False False  True False False  True False False False  True\n"," False False False False False False False False False False False False\n"," False False False False False False  True False  True False False False\n"," False False False False]\n","[55  1  1 22 19 23  3 30 41 24 40 46 53 34 39 36 38  1  1 18  7 48 13 28\n"," 14  1  8 20  1 35 27  1 31 33 47  1 43 10 15 16 32  2 44 37 29 17 49 54\n"," 12  6  9 21 45 52  1 42  1 11 50 25  5  4 51 26]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JfxS6drDEObM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1594326556292,"user_tz":240,"elapsed":68401,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"4d2c1698-6a5d-41fe-a03c-4250533e1203"},"source":["x = model_data.drop(['A', 'B', 'C', 'FTHG'],axis = 1)\n","y = model_data.A\n","x1 = model_data.drop(['A', 'B', 'C', 'FTHG'],axis = 1)\n","y1 = model_data.B\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.feature_selection import RFECV\n","\n","logreg_model = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logreg_model_1 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","rfecv = RFECV(estimator=logreg_model, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n","rfecv_1 = RFECV(estimator=logreg_model_1, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n","rfecv.fit(x, y)\n","rfecv_1.fit(x1, y1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RFECV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n","      estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n","                                   fit_intercept=True, intercept_scaling=1,\n","                                   l1_ratio=None, max_iter=100,\n","                                   multi_class='ovr', n_jobs=None, penalty='l2',\n","                                   random_state=None, solver='newton-cg',\n","                                   tol=0.0001, verbose=0, warm_start=False),\n","      min_features_to_select=1, n_jobs=None, scoring='accuracy', step=1,\n","      verbose=0)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"U8z6oqa7ERkH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594327414237,"user_tz":240,"elapsed":456,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"d240aab7-d59f-467a-a960-73ffb9ab6116"},"source":["print('Optimal number of features: {}'.format(rfecv.n_features_))\n","print('Optimal number of features: {}'.format(rfecv_1.n_features_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Optimal number of features: 47\n","Optimal number of features: 46\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rghFbKHnEaML","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":608},"executionInfo":{"status":"ok","timestamp":1594305937169,"user_tz":240,"elapsed":853,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"f8237315-5064-49a5-e1a5-3752c2503702"},"source":["plt.figure(figsize=(16, 9))\n","plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n","plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n","plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n","plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n","\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1152x648 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"cOxMvgIGcJwR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1594305940349,"user_tz":240,"elapsed":377,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"5e315788-427b-4f62-fe4f-130960ffbaef"},"source":["rfecv.grid_scores_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.75818042, 0.83063812, 0.83377537, 0.83730632, 0.83769617,\n","       0.84671501, 0.84553624, 0.84984767, 0.85573002, 0.87062354,\n","       0.86866199, 0.86709259, 0.86552396, 0.86866199, 0.8694463 ,\n","       0.86748475, 0.86787844, 0.86866352, 0.86748705, 0.86748782,\n","       0.86670197, 0.86630904, 0.86356471, 0.86356395, 0.86277963,\n","       0.8627804 , 0.86121177, 0.86238824, 0.86317332, 0.86278117,\n","       0.86317256, 0.86434979, 0.86552627, 0.86434979, 0.86591842,\n","       0.86435133, 0.86395917, 0.8655278 , 0.86395994, 0.86631211,\n","       0.86709566, 0.87141015, 0.87297648, 0.87258432, 0.87297648,\n","       0.87532712, 0.87532712, 0.87375772, 0.87336557, 0.87375772,\n","       0.87297341, 0.87141015, 0.87023445, 0.87023445, 0.86945014,\n","       0.86945014, 0.87023522, 0.87062737, 0.87101953, 0.87180384,\n","       0.8733717 , 0.872196  , 0.87258816, 0.86945167])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"j05S2hNV_5se","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594305973120,"user_tz":240,"elapsed":327,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"4c19ca61-deb4-45ef-9caf-955e52f7d1d7"},"source":["g_scores = rfecv.grid_scores_\n","indices = np.argsort(g_scores)[::-1]\n","print('Printing RFECV results:')\n","for f in range(x.shape[1]):\n","    print(\"%d. Number of features: %d; Grid_Score: %f\" % (f + 1, indices[f]+1, g_scores[indices[f]]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Printing RFECV results:\n","1. Number of features: 46; Grid_Score: 0.875327\n","2. Number of features: 47; Grid_Score: 0.875327\n","3. Number of features: 48; Grid_Score: 0.873758\n","4. Number of features: 50; Grid_Score: 0.873758\n","5. Number of features: 61; Grid_Score: 0.873372\n","6. Number of features: 49; Grid_Score: 0.873366\n","7. Number of features: 43; Grid_Score: 0.872976\n","8. Number of features: 45; Grid_Score: 0.872976\n","9. Number of features: 51; Grid_Score: 0.872973\n","10. Number of features: 63; Grid_Score: 0.872588\n","11. Number of features: 44; Grid_Score: 0.872584\n","12. Number of features: 62; Grid_Score: 0.872196\n","13. Number of features: 60; Grid_Score: 0.871804\n","14. Number of features: 52; Grid_Score: 0.871410\n","15. Number of features: 42; Grid_Score: 0.871410\n","16. Number of features: 59; Grid_Score: 0.871020\n","17. Number of features: 58; Grid_Score: 0.870627\n","18. Number of features: 10; Grid_Score: 0.870624\n","19. Number of features: 57; Grid_Score: 0.870235\n","20. Number of features: 53; Grid_Score: 0.870234\n","21. Number of features: 54; Grid_Score: 0.870234\n","22. Number of features: 64; Grid_Score: 0.869452\n","23. Number of features: 56; Grid_Score: 0.869450\n","24. Number of features: 55; Grid_Score: 0.869450\n","25. Number of features: 15; Grid_Score: 0.869446\n","26. Number of features: 18; Grid_Score: 0.868664\n","27. Number of features: 14; Grid_Score: 0.868662\n","28. Number of features: 11; Grid_Score: 0.868662\n","29. Number of features: 17; Grid_Score: 0.867878\n","30. Number of features: 20; Grid_Score: 0.867488\n","31. Number of features: 19; Grid_Score: 0.867487\n","32. Number of features: 16; Grid_Score: 0.867485\n","33. Number of features: 41; Grid_Score: 0.867096\n","34. Number of features: 12; Grid_Score: 0.867093\n","35. Number of features: 21; Grid_Score: 0.866702\n","36. Number of features: 40; Grid_Score: 0.866312\n","37. Number of features: 22; Grid_Score: 0.866309\n","38. Number of features: 35; Grid_Score: 0.865918\n","39. Number of features: 38; Grid_Score: 0.865528\n","40. Number of features: 33; Grid_Score: 0.865526\n","41. Number of features: 13; Grid_Score: 0.865524\n","42. Number of features: 36; Grid_Score: 0.864351\n","43. Number of features: 32; Grid_Score: 0.864350\n","44. Number of features: 34; Grid_Score: 0.864350\n","45. Number of features: 39; Grid_Score: 0.863960\n","46. Number of features: 37; Grid_Score: 0.863959\n","47. Number of features: 23; Grid_Score: 0.863565\n","48. Number of features: 24; Grid_Score: 0.863564\n","49. Number of features: 29; Grid_Score: 0.863173\n","50. Number of features: 31; Grid_Score: 0.863173\n","51. Number of features: 30; Grid_Score: 0.862781\n","52. Number of features: 26; Grid_Score: 0.862780\n","53. Number of features: 25; Grid_Score: 0.862780\n","54. Number of features: 28; Grid_Score: 0.862388\n","55. Number of features: 27; Grid_Score: 0.861212\n","56. Number of features: 9; Grid_Score: 0.855730\n","57. Number of features: 8; Grid_Score: 0.849848\n","58. Number of features: 6; Grid_Score: 0.846715\n","59. Number of features: 7; Grid_Score: 0.845536\n","60. Number of features: 5; Grid_Score: 0.837696\n","61. Number of features: 4; Grid_Score: 0.837306\n","62. Number of features: 3; Grid_Score: 0.833775\n","63. Number of features: 2; Grid_Score: 0.830638\n","64. Number of features: 1; Grid_Score: 0.758180\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ek-9CS4Cc7IX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1594327423010,"user_tz":240,"elapsed":984,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"900ddcf6-00ce-45bd-d928-78897a765276"},"source":["print(rfecv.support_)\n","print(rfecv.ranking_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[False  True  True  True  True  True  True  True False  True False False\n"," False  True False  True  True  True  True  True  True False  True  True\n","  True  True  True  True  True  True  True  True  True  True False  True\n"," False  True  True  True  True  True False  True  True  True False False\n","  True  True  True  True False False  True False  True  True False  True\n","  True  True False  True]\n","[18  1  1  1  1  1  1  1  4  1  3  9 16  1  2  1  1  1  1  1  1 11  1  1\n","  1  1  1  1  1  1  1  1  1  1 10  1  6  1  1  1  1  1  7  1  1  1 12 17\n","  1  1  1  1  8 15  1  5  1  1 13  1  1  1 14  1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nh1OdyJyOy1y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1594327426169,"user_tz":240,"elapsed":471,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"1f7f27e7-7deb-4e02-f42b-add0922367fc"},"source":["print(rfecv_1.support_)\n","print(rfecv_1.ranking_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[False  True  True  True  True  True False  True  True  True  True False\n"," False False False  True  True  True  True False  True False  True  True\n","  True  True  True False  True  True  True  True  True  True  True  True\n","  True  True  True False  True  True False  True  True  True False  True\n","  True False False  True  True False  True  True  True  True False  True\n"," False  True False  True]\n","[19  1  1  1  1  1  9  1  1  1  1  8 18  6 11  1  1  1  1  3  1 16  1  1\n","  1  1  1 15  1  1  1  1  1  1  1  1  1  1  1 13  1  1 17  1  1  1  5  1\n","  1 10  7  1  1  4  1  1  1  1 14  1 12  1  2  1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qaP2uuZUMxMU","colab_type":"code","colab":{}},"source":["rfecv_support_array=[]\n","rfecv_1_support_array=[]\n","for i in range(0,64):\n","    if rfecv.support_[i] == 'False':\n","        rfecv_support_array.append('1')\n","    elif rfecv.support_[i] == 'True':\n","        rfecv_support_array.append('0')\n","for i in range(0,64):\n","  if rfecv_1.support_[i] == 'False':\n","    rfecv_1_support_array.append('1')\n","  elif rfecv_1.support_[i] == 'True':\n","    rfecv_1_support_array.append('0')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"erRX4NCOUli1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594327495945,"user_tz":240,"elapsed":367,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"77a12e5f-a2ee-4cca-f25b-c22209080511"},"source":["rfecv_support_array"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"TU6oCDbpLFag","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593725538875,"user_tz":240,"elapsed":618,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"cf44eaa9-6bf5-478c-fc39-4553b2e09104"},"source":["FF = list(x.columns.values.tolist())\n","for i in range(len(FF)):\n","  print(i,end=\" \")\n","  print(FF[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 Unnamed: 0\n","1 FTAG\n","2 HTHG\n","3 HTAG\n","4 D\n","5 E\n","6 F\n","7 HS\n","8 AS\n","9 HST\n","10 AST\n","11 HF\n","12 AF\n","13 HC\n","14 AC\n","15 HY\n","16 AY\n","17 HR\n","18 AR\n","19 B365H\n","20 B365D\n","21 B365A\n","22 BWH\n","23 BWD\n","24 BWA\n","25 GBH\n","26 GBD\n","27 GBA\n","28 IWH\n","29 IWD\n","30 IWA\n","31 LBH\n","32 LBD\n","33 LBA\n","34 SBH\n","35 SBD\n","36 SBA\n","37 WHH\n","38 WHD\n","39 WHA\n","40 SJH\n","41 SJD\n","42 SJA\n","43 VCH\n","44 VCD\n","45 VCA\n","46 Bb1X2\n","47 BbMxH\n","48 BbAvH\n","49 BbMxD\n","50 BbAvD\n","51 BbMxA\n","52 BbAvA\n","53 BbOU\n","54 BbMx>2.5\n","55 BbAv>2.5\n","56 BbMx<2.5\n","57 BbAv<2.5\n","58 BbAH\n","59 BbAHh\n","60 BbMxAHH\n","61 BbAvAHH\n","62 BbMxAHA\n","63 BbAvAHA\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gai4ojfZ0l-J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":316},"executionInfo":{"status":"error","timestamp":1593620395213,"user_tz":240,"elapsed":1181,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"8cc58674-248b-496d-ceb3-3c4c1c0dba8d"},"source":["print(np.where(rfecv.support_ == False)[0])\n","x.drop(x.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 0  8 10 11 12 14 21 34 36 42 46 47 52 53 55 58 62]\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-d6b030b687ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3940\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3941\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3943\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 47 is out of bounds for axis 0 with size 47"]}]},{"cell_type":"code","metadata":{"id":"XO-O3O3q2FLL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593616863847,"user_tz":240,"elapsed":955,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"246f53b0-e14d-4082-d45f-d357b3963ce2"},"source":["len(x.columns)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["47"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"viqXY9kImfQw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":819},"executionInfo":{"status":"ok","timestamp":1593616867109,"user_tz":240,"elapsed":915,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"f9400e9f-d7b3-472d-8f50-b115ddbddb17"},"source":["importance_array=[]\n","importance_array=[(np.absolute(rfecv.estimator_.coef_))]\n","importance_array = np.reshape(importance_array,(47,1))\n","importance_array"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.95935036],\n","       [1.75378424],\n","       [0.19269361],\n","       [0.38874366],\n","       [0.14632958],\n","       [0.24244054],\n","       [0.08278448],\n","       [0.27066217],\n","       [0.06679599],\n","       [0.0635786 ],\n","       [0.03381155],\n","       [0.7146516 ],\n","       [0.57012765],\n","       [0.17638582],\n","       [0.32847251],\n","       [0.36663718],\n","       [0.1456743 ],\n","       [0.26603017],\n","       [0.30697165],\n","       [0.28711688],\n","       [0.1842602 ],\n","       [0.17128122],\n","       [0.08801925],\n","       [0.07854637],\n","       [0.61436271],\n","       [0.19134668],\n","       [0.09943328],\n","       [0.14204409],\n","       [0.2457527 ],\n","       [0.42171523],\n","       [0.13640964],\n","       [0.10566396],\n","       [0.47222303],\n","       [0.04070596],\n","       [0.08917292],\n","       [0.26810996],\n","       [0.28629683],\n","       [0.40445194],\n","       [0.40240978],\n","       [0.20248219],\n","       [0.58789869],\n","       [0.42303325],\n","       [0.34273123],\n","       [0.15776969],\n","       [0.45885186],\n","       [0.58856798],\n","       [0.09917139]])"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"wWqJkINvp_i9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593616871004,"user_tz":240,"elapsed":656,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"526aad77-e929-47ac-e187-d588f8ceb2aa"},"source":["len(importance_array)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["47"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"_lbQ4gksmybU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":877},"executionInfo":{"status":"ok","timestamp":1593616873244,"user_tz":240,"elapsed":1406,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"8c12f249-46ab-48fa-8672-d2869d86bfdc"},"source":["dset = pd.DataFrame()\n","dset['attr'] = x.columns\n","dset['importance'] = importance_array\n","\n","dset = dset.sort_values(by='importance', ascending=False)\n","\n","\n","plt.figure(figsize=(16, 14))\n","plt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\n","plt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\n","plt.xlabel('Importance', fontsize=14, labelpad=20)\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1152x1008 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"5xGe2Ji66Aff","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594335956207,"user_tz":240,"elapsed":4992,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"08735053-3476-452b-f730-fb8b6f6d91d2"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","model_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Summer Research/Data/model_data.csv')\n","col_name_A1= ['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9']\n","#col_name_A2 = ['a5', 'a6', 'a7', 'a8', 'a9']\n","col_name_B1= ['b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9']\n","#col_name_B2 = ['b5', 'b6', 'b7', 'b8', 'b9']\n","col_name_C1 = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n","#col_name_C2 = ['c5', 'c6', 'c7', 'c8', 'c9']\n","#model_data1 = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","# Data For Model A\n","x = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","x = x.iloc[:, 1:]\n","y = model_data.A\n","# Data for Model B\n","x1 = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA','AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","x1 = x1.iloc[:, 1:]\n","y1 = model_data.B\n","# Data for model C\n","x2 = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","x2 = x2.iloc[:, 1:]\n","y2 = model_data.C\n","\n","for i in range(0,10):\n","    # For model A\n","    y = model_data.A\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)\n","    logistic_regression = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","    logistic_regression.fit(x_train,y_train)\n","    y_pred = logistic_regression.predict(x_test)\n","    y_train_predict = logistic_regression.predict(x_train)\n","    #for model B\n","    x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.3, random_state=4)\n","    logistic_regression1 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","    logistic_regression1.fit(x1_train,y1_train)\n","    y1_pred = logistic_regression1.predict(x1_test)\n","    y1_train_predict = logistic_regression1.predict(x1_train)\n","    #for model C\n","    x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.3, random_state=4)\n","    logistic_regression2 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","    logistic_regression2.fit(x2_train,y2_train)\n","    y2_pred = logistic_regression2.predict(x2_test)\n","    y2_train_predict = logistic_regression2.predict(x2_train)\n","    #from sklearn.preprocessing import MinMaxScaler\n","    #scaler = MinMaxScaler(feature_range = (0,1))\n","    #scaler.fit(x5_train)\n","\n","  ##appending o/p array and adding data to the model\n","    # For Model A \n","    model_data[col_name_A1[i]]=np.append(y_pred, y_train_predict)\n","    #model_data[col_name_A2[i]]=np.append(y2_pred, y2_train_predict)\n","    # For Model B \n","    model_data[col_name_B1[i]]=np.append(y1_pred, y1_train_predict)\n","    #model_data[col_name_B2[i]]=np.append(y2_pred, y2_train_predict)\n","    # fOR MODEL C \n","    model_data[col_name_C1[i]]=np.append(y2_pred, y2_train_predict)\n","    #model_data[col_name_C2[i]]=np.append(y1_pred, y1_train_predict)\n","  \n","  ##printing accuracy FOR ALL THE MODELS\n","    # ACCURACY FOR MODEL A\n","    accuracy_test = metrics.accuracy_score(y_test, y_pred)\n","    accuracy_percentage_test = 100 * accuracy_test\n","    print (\"Test data accuracy for model A: \", accuracy_percentage_test)\n","    accuracy_train = metrics.accuracy_score(y_train, y_train_predict)\n","    accuracy_percentage_train = 100 * accuracy_train\n","    print (\"Train data Accuracy for model A:\",accuracy_percentage_train)\n","    print (\" \")\n","    # ACCURACY FOR MODEL B\n","    accuracy_test_1 = metrics.accuracy_score(y1_test, y1_pred)\n","    accuracy_percentage_test_1 = 100 * accuracy_test_1\n","    print (\"Test data accuracy for model B: \", accuracy_percentage_test_1)\n","    accuracy_train_1 = metrics.accuracy_score(y1_train, y1_train_predict)\n","    accuracy_percentage_train_1 = 100 * accuracy_train_1\n","    print (\"Train data Accuracy for model B:\",accuracy_percentage_train_1)\n","    print (\" \")\n","    # ACCURACY FOR MODEL C\n","    accuracy_test_2 = metrics.accuracy_score(y2_test, y2_pred)\n","    accuracy_percentage_test_2 = 100 * accuracy_test_2\n","    print (\"Test data accuracy for model C: \",accuracy_percentage_test_2)\n","    accuracy_train_2 = metrics.accuracy_score(y2_train, y2_train_predict)\n","    accuracy_percentage_train_2 = 100 * accuracy_train_2\n","    print (\"Train data Accuracy for model C:\",accuracy_percentage_train_2)\n","    print(\"  \")\n","    print (\" End of cycle:\",i)\n","    # Data For Model A\n","    #x = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","    #x = x.iloc[:, 1:]\n","    x[col_name_B1[i]]=model_data[col_name_B1[i]]\n","    x[col_name_C1[i]]=model_data[col_name_C1[i]]\n","    #y = model_data.A\n","    # Data for Model B\n","    #x1 = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","    #x1 = x1.iloc[:, 1:]\n","    x1[col_name_A1[i]] = model_data[col_name_A1[i]]\n","    x1[col_name_C1[i]] = model_data[col_name_C1[i]]\n","    y1 = model_data.B\n","    # Data for model C\n","    #x2 = model_data.drop(['A', 'B', 'C','FTHG', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD'],axis = 1)\n","    #x2 = x2.iloc[:, 1:]\n","    x2[col_name_A1[i]]=model_data[col_name_A1[i]]\n","    x2[col_name_B1[i]]=model_data[col_name_B1[i]]\n","    y2 = model_data.C\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test data accuracy for model A:  80.80939947780679\n","Train data Accuracy for model A: 82.74509803921568\n"," \n","Test data accuracy for model B:  87.20626631853786\n","Train data Accuracy for model B: 88.79551820728291\n"," \n","Test data accuracy for model C:  65.14360313315926\n","Train data Accuracy for model C: 63.97759103641456\n","  \n"," End of cycle: 0\n","Test data accuracy for model A:  81.20104438642298\n","Train data Accuracy for model A: 82.24089635854341\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 88.90756302521008\n"," \n","Test data accuracy for model C:  65.66579634464752\n","Train data Accuracy for model C: 64.14565826330532\n","  \n"," End of cycle: 1\n","Test data accuracy for model A:  81.4621409921671\n","Train data Accuracy for model A: 82.52100840336134\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 88.96358543417367\n"," \n","Test data accuracy for model C:  65.66579634464752\n","Train data Accuracy for model C: 64.08963585434174\n","  \n"," End of cycle: 2\n","Test data accuracy for model A:  81.33159268929504\n","Train data Accuracy for model A: 82.52100840336134\n"," \n","Test data accuracy for model B:  87.46736292428199\n","Train data Accuracy for model B: 89.01960784313725\n"," \n","Test data accuracy for model C:  65.53524804177546\n","Train data Accuracy for model C: 64.2016806722689\n","  \n"," End of cycle: 3\n","Test data accuracy for model A:  81.33159268929504\n","Train data Accuracy for model A: 82.40896358543417\n"," \n","Test data accuracy for model B:  87.46736292428199\n","Train data Accuracy for model B: 89.07563025210085\n"," \n","Test data accuracy for model C:  65.66579634464752\n","Train data Accuracy for model C: 64.25770308123249\n","  \n"," End of cycle: 4\n","Test data accuracy for model A:  81.4621409921671\n","Train data Accuracy for model A: 82.296918767507\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 89.07563025210085\n"," \n","Test data accuracy for model C:  65.66579634464752\n","Train data Accuracy for model C: 64.25770308123249\n","  \n"," End of cycle: 5\n","Test data accuracy for model A:  81.4621409921671\n","Train data Accuracy for model A: 82.296918767507\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 89.07563025210085\n"," \n","Test data accuracy for model C:  65.66579634464752\n","Train data Accuracy for model C: 64.2016806722689\n","  \n"," End of cycle: 6\n","Test data accuracy for model A:  81.4621409921671\n","Train data Accuracy for model A: 82.35294117647058\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 89.07563025210085\n"," \n","Test data accuracy for model C:  65.66579634464752\n","Train data Accuracy for model C: 64.2016806722689\n","  \n"," End of cycle: 7\n","Test data accuracy for model A:  81.4621409921671\n","Train data Accuracy for model A: 82.35294117647058\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 89.07563025210085\n"," \n","Test data accuracy for model C:  65.53524804177546\n","Train data Accuracy for model C: 64.14565826330532\n","  \n"," End of cycle: 8\n","Test data accuracy for model A:  81.4621409921671\n","Train data Accuracy for model A: 82.35294117647058\n"," \n","Test data accuracy for model B:  87.33681462140991\n","Train data Accuracy for model B: 89.07563025210085\n"," \n","Test data accuracy for model C:  65.40469973890339\n","Train data Accuracy for model C: 64.14565826330532\n","  \n"," End of cycle: 9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mxCTJF9iGVV2","colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"status":"ok","timestamp":1594336160245,"user_tz":240,"elapsed":512,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"6711929c-8188-449c-ba81-af81a13d012d"},"source":["model_data = model_data.iloc[:, :-15]\n","model_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>FTHG</th>\n","      <th>FTAG</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>HS</th>\n","      <th>AS</th>\n","      <th>HST</th>\n","      <th>AST</th>\n","      <th>HF</th>\n","      <th>AF</th>\n","      <th>HC</th>\n","      <th>AC</th>\n","      <th>HY</th>\n","      <th>AY</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>B365A</th>\n","      <th>BWH</th>\n","      <th>BWD</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBD</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>IWD</th>\n","      <th>IWA</th>\n","      <th>LBH</th>\n","      <th>LBD</th>\n","      <th>LBA</th>\n","      <th>SBH</th>\n","      <th>SBD</th>\n","      <th>...</th>\n","      <th>WHA</th>\n","      <th>SJH</th>\n","      <th>SJD</th>\n","      <th>SJA</th>\n","      <th>VCH</th>\n","      <th>VCD</th>\n","      <th>VCA</th>\n","      <th>Bb1X2</th>\n","      <th>BbMxH</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbAvA</th>\n","      <th>BbOU</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbAv&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAH</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>BbMxAHA</th>\n","      <th>BbAvAHA</th>\n","      <th>a0</th>\n","      <th>b0</th>\n","      <th>c0</th>\n","      <th>a1</th>\n","      <th>b1</th>\n","      <th>c1</th>\n","      <th>a2</th>\n","      <th>b2</th>\n","      <th>c2</th>\n","      <th>a3</th>\n","      <th>b3</th>\n","      <th>c3</th>\n","      <th>a4</th>\n","      <th>b4</th>\n","      <th>c4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>14.0</td>\n","      <td>16.0</td>\n","      <td>7.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>3.00</td>\n","      <td>2.10</td>\n","      <td>3.25</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.20</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>3.0</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.200</td>\n","      <td>...</td>\n","      <td>2.80</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.40</td>\n","      <td>2.20</td>\n","      <td>3.25</td>\n","      <td>3.10</td>\n","      <td>56.0</td>\n","      <td>2.40</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>3.05</td>\n","      <td>36.0</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>22.0</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>1.92</td>\n","      <td>1.84</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>10.0</td>\n","      <td>12.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>15.0</td>\n","      <td>14.0</td>\n","      <td>8.0</td>\n","      <td>6.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>1.72</td>\n","      <td>4.35</td>\n","      <td>3.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>3.40</td>\n","      <td>1.83</td>\n","      <td>3.80</td>\n","      <td>3.1</td>\n","      <td>1.80</td>\n","      <td>3.75</td>\n","      <td>3.20</td>\n","      <td>1.83</td>\n","      <td>4.33</td>\n","      <td>3.600</td>\n","      <td>...</td>\n","      <td>1.72</td>\n","      <td>4.00</td>\n","      <td>3.25</td>\n","      <td>1.83</td>\n","      <td>4.50</td>\n","      <td>3.30</td>\n","      <td>1.80</td>\n","      <td>56.0</td>\n","      <td>5.65</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>23.0</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1.93</td>\n","      <td>1.86</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>4.0</td>\n","      <td>12.0</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.87</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>3.30</td>\n","      <td>2.95</td>\n","      <td>2.20</td>\n","      <td>3.0</td>\n","      <td>2.90</td>\n","      <td>2.25</td>\n","      <td>3.00</td>\n","      <td>2.88</td>\n","      <td>2.30</td>\n","      <td>3.200</td>\n","      <td>...</td>\n","      <td>2.62</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>3.00</td>\n","      <td>2.35</td>\n","      <td>3.25</td>\n","      <td>2.80</td>\n","      <td>56.0</td>\n","      <td>2.60</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.87</td>\n","      <td>36.0</td>\n","      <td>2.24</td>\n","      <td>2.04</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>21.0</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>2.11</td>\n","      <td>2.05</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","      <td>13.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","      <td>3.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>3.45</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>3.45</td>\n","      <td>4.75</td>\n","      <td>1.70</td>\n","      <td>3.2</td>\n","      <td>4.20</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>4.50</td>\n","      <td>1.70</td>\n","      <td>3.400</td>\n","      <td>...</td>\n","      <td>4.33</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>1.75</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>55.0</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>4.79</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.94</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>23.0</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1.83</td>\n","      <td>1.76</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.40</td>\n","      <td>2.90</td>\n","      <td>3.35</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>3.30</td>\n","      <td>2.45</td>\n","      <td>2.50</td>\n","      <td>3.0</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.70</td>\n","      <td>3.200</td>\n","      <td>...</td>\n","      <td>2.30</td>\n","      <td>2.75</td>\n","      <td>3.20</td>\n","      <td>2.38</td>\n","      <td>2.80</td>\n","      <td>3.25</td>\n","      <td>2.35</td>\n","      <td>56.0</td>\n","      <td>3.30</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.35</td>\n","      <td>36.0</td>\n","      <td>2.23</td>\n","      <td>2.02</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>21.0</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>2.04</td>\n","      <td>2.00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2546</th>\n","      <td>375</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6.0</td>\n","      <td>17.0</td>\n","      <td>5.0</td>\n","      <td>9.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.44</td>\n","      <td>6.75</td>\n","      <td>4.60</td>\n","      <td>1.42</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>6.00</td>\n","      <td>4.0</td>\n","      <td>1.47</td>\n","      <td>7.50</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.600</td>\n","      <td>...</td>\n","      <td>1.44</td>\n","      <td>7.50</td>\n","      <td>4.80</td>\n","      <td>1.40</td>\n","      <td>8.00</td>\n","      <td>4.80</td>\n","      <td>1.44</td>\n","      <td>39.0</td>\n","      <td>8.30</td>\n","      <td>7.23</td>\n","      <td>4.90</td>\n","      <td>4.51</td>\n","      <td>1.47</td>\n","      <td>1.43</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.60</td>\n","      <td>2.43</td>\n","      <td>2.27</td>\n","      <td>20.0</td>\n","      <td>1.25</td>\n","      <td>1.95</td>\n","      <td>1.89</td>\n","      <td>2.03</td>\n","      <td>1.97</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2547</th>\n","      <td>376</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>13.0</td>\n","      <td>14.0</td>\n","      <td>8.0</td>\n","      <td>9.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.40</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.30</td>\n","      <td>3.60</td>\n","      <td>2.05</td>\n","      <td>3.25</td>\n","      <td>3.30</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.2</td>\n","      <td>2.20</td>\n","      <td>3.40</td>\n","      <td>3.40</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.500</td>\n","      <td>...</td>\n","      <td>2.15</td>\n","      <td>3.25</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.60</td>\n","      <td>3.60</td>\n","      <td>2.10</td>\n","      <td>39.0</td>\n","      <td>3.66</td>\n","      <td>3.38</td>\n","      <td>3.72</td>\n","      <td>3.45</td>\n","      <td>2.20</td>\n","      <td>2.10</td>\n","      <td>33.0</td>\n","      <td>1.77</td>\n","      <td>1.71</td>\n","      <td>2.24</td>\n","      <td>2.11</td>\n","      <td>20.0</td>\n","      <td>0.25</td>\n","      <td>2.13</td>\n","      <td>2.06</td>\n","      <td>1.84</td>\n","      <td>1.81</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2548</th>\n","      <td>377</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>15.0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>7.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>8.50</td>\n","      <td>1.40</td>\n","      <td>4.40</td>\n","      <td>7.75</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.00</td>\n","      <td>1.37</td>\n","      <td>4.4</td>\n","      <td>7.30</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>4.333</td>\n","      <td>...</td>\n","      <td>9.00</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>5.00</td>\n","      <td>9.00</td>\n","      <td>39.0</td>\n","      <td>1.41</td>\n","      <td>1.40</td>\n","      <td>5.07</td>\n","      <td>4.55</td>\n","      <td>9.10</td>\n","      <td>8.04</td>\n","      <td>32.0</td>\n","      <td>1.68</td>\n","      <td>1.63</td>\n","      <td>2.45</td>\n","      <td>2.23</td>\n","      <td>18.0</td>\n","      <td>-1.25</td>\n","      <td>1.93</td>\n","      <td>1.90</td>\n","      <td>2.02</td>\n","      <td>1.97</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2549</th>\n","      <td>378</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>12.0</td>\n","      <td>12.0</td>\n","      <td>8.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.62</td>\n","      <td>5.00</td>\n","      <td>3.75</td>\n","      <td>1.65</td>\n","      <td>5.40</td>\n","      <td>3.6</td>\n","      <td>1.57</td>\n","      <td>4.50</td>\n","      <td>3.75</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>3.750</td>\n","      <td>...</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>5.25</td>\n","      <td>4.20</td>\n","      <td>1.67</td>\n","      <td>39.0</td>\n","      <td>5.60</td>\n","      <td>5.02</td>\n","      <td>4.28</td>\n","      <td>3.93</td>\n","      <td>1.70</td>\n","      <td>1.65</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.58</td>\n","      <td>2.50</td>\n","      <td>2.31</td>\n","      <td>18.0</td>\n","      <td>0.75</td>\n","      <td>2.14</td>\n","      <td>2.08</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>379</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>14.0</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>7.0</td>\n","      <td>15.0</td>\n","      <td>9.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>1.60</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>5.75</td>\n","      <td>1.60</td>\n","      <td>3.6</td>\n","      <td>5.00</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>6.00</td>\n","      <td>1.55</td>\n","      <td>4.000</td>\n","      <td>...</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>4.30</td>\n","      <td>6.00</td>\n","      <td>39.0</td>\n","      <td>1.62</td>\n","      <td>1.57</td>\n","      <td>4.35</td>\n","      <td>3.95</td>\n","      <td>6.40</td>\n","      <td>5.80</td>\n","      <td>28.0</td>\n","      <td>1.67</td>\n","      <td>1.59</td>\n","      <td>2.43</td>\n","      <td>2.26</td>\n","      <td>22.0</td>\n","      <td>-1.00</td>\n","      <td>2.01</td>\n","      <td>1.96</td>\n","      <td>1.97</td>\n","      <td>1.90</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2551 rows × 83 columns</p>\n","</div>"],"text/plain":["      Unnamed: 0  FTHG  FTAG  A  B  C  HTHG  ...  c2  a3  b3  c3  a4  b4  c4\n","0              0   2.0   2.0  0  0  1   2.0  ...   0   0   1   0   0   1   0\n","1              1   0.0   2.0  0  1  0   0.0  ...   1   1   0   1   1   0   1\n","2              2   0.0   0.0  0  0  1   0.0  ...   1   0   0   1   0   0   1\n","3              3   0.0   0.0  0  0  1   0.0  ...   0   1   0   0   1   0   0\n","4              4   0.0   0.0  0  0  1   0.0  ...   1   0   0   1   0   0   1\n","...          ...   ...   ... .. .. ..   ...  ...  ..  ..  ..  ..  ..  ..  ..\n","2546         375   0.0   1.0  0  1  0   0.0  ...   0   0   1   0   0   1   0\n","2547         376   1.0   0.0  1  0  0   0.0  ...   0   1   0   0   1   0   0\n","2548         377   2.0   0.0  1  0  0   1.0  ...   0   1   0   0   1   0   0\n","2549         378   2.0   3.0  0  1  0   2.0  ...   1   1   0   1   1   0   1\n","2550         379   3.0   2.0  1  0  0   2.0  ...   1   1   0   1   1   0   1\n","\n","[2551 rows x 83 columns]"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"ebj0buqN1kB-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1594336104300,"user_tz":240,"elapsed":419,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"6acdefc5-34cd-4154-831c-7e1bc09c38cf"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FTAG</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>HST</th>\n","      <th>AF</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>BWH</th>\n","      <th>BWD</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>LBH</th>\n","      <th>LBD</th>\n","      <th>LBA</th>\n","      <th>SBH</th>\n","      <th>SBD</th>\n","      <th>WHH</th>\n","      <th>WHD</th>\n","      <th>WHA</th>\n","      <th>SJD</th>\n","      <th>VCD</th>\n","      <th>VCA</th>\n","      <th>BbMxH</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbAvA</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbAv&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>BbAvAHA</th>\n","      <th>b0</th>\n","      <th>c0</th>\n","      <th>b1</th>\n","      <th>c1</th>\n","      <th>b2</th>\n","      <th>c2</th>\n","      <th>b3</th>\n","      <th>c3</th>\n","      <th>b4</th>\n","      <th>c4</th>\n","      <th>b5</th>\n","      <th>c5</th>\n","      <th>b6</th>\n","      <th>c6</th>\n","      <th>b7</th>\n","      <th>c7</th>\n","      <th>b8</th>\n","      <th>c8</th>\n","      <th>b9</th>\n","      <th>c9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>16.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>2.10</td>\n","      <td>3.25</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.200</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>3.25</td>\n","      <td>3.25</td>\n","      <td>3.10</td>\n","      <td>2.40</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>3.05</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>1.84</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>14.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>4.35</td>\n","      <td>3.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>1.83</td>\n","      <td>3.80</td>\n","      <td>3.75</td>\n","      <td>3.20</td>\n","      <td>1.83</td>\n","      <td>4.33</td>\n","      <td>3.600</td>\n","      <td>4.33</td>\n","      <td>3.20</td>\n","      <td>1.72</td>\n","      <td>3.25</td>\n","      <td>3.30</td>\n","      <td>1.80</td>\n","      <td>5.65</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>2.10</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1.86</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7.0</td>\n","      <td>13.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>2.95</td>\n","      <td>2.20</td>\n","      <td>2.25</td>\n","      <td>3.00</td>\n","      <td>2.88</td>\n","      <td>2.30</td>\n","      <td>3.200</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.62</td>\n","      <td>3.20</td>\n","      <td>3.25</td>\n","      <td>2.80</td>\n","      <td>2.60</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.87</td>\n","      <td>2.24</td>\n","      <td>2.04</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>2.05</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>1.65</td>\n","      <td>3.45</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>4.75</td>\n","      <td>1.70</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>4.50</td>\n","      <td>1.70</td>\n","      <td>3.400</td>\n","      <td>1.70</td>\n","      <td>3.30</td>\n","      <td>4.33</td>\n","      <td>3.25</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>4.79</td>\n","      <td>2.10</td>\n","      <td>1.94</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1.76</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.90</td>\n","      <td>3.35</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>2.45</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.70</td>\n","      <td>3.200</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>3.25</td>\n","      <td>2.35</td>\n","      <td>3.30</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.35</td>\n","      <td>2.23</td>\n","      <td>2.02</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>2.00</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2546</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>6.75</td>\n","      <td>4.60</td>\n","      <td>1.42</td>\n","      <td>7.00</td>\n","      <td>1.40</td>\n","      <td>6.00</td>\n","      <td>7.50</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.600</td>\n","      <td>7.00</td>\n","      <td>4.33</td>\n","      <td>1.44</td>\n","      <td>4.80</td>\n","      <td>4.80</td>\n","      <td>1.44</td>\n","      <td>8.30</td>\n","      <td>7.23</td>\n","      <td>4.90</td>\n","      <td>4.51</td>\n","      <td>1.47</td>\n","      <td>1.43</td>\n","      <td>1.67</td>\n","      <td>1.60</td>\n","      <td>2.43</td>\n","      <td>2.27</td>\n","      <td>1.25</td>\n","      <td>1.95</td>\n","      <td>1.89</td>\n","      <td>1.97</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2547</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.40</td>\n","      <td>3.50</td>\n","      <td>3.30</td>\n","      <td>3.60</td>\n","      <td>2.05</td>\n","      <td>3.25</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.40</td>\n","      <td>3.40</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.500</td>\n","      <td>3.40</td>\n","      <td>3.30</td>\n","      <td>2.15</td>\n","      <td>3.50</td>\n","      <td>3.60</td>\n","      <td>2.10</td>\n","      <td>3.66</td>\n","      <td>3.38</td>\n","      <td>3.72</td>\n","      <td>3.45</td>\n","      <td>2.20</td>\n","      <td>2.10</td>\n","      <td>1.77</td>\n","      <td>1.71</td>\n","      <td>2.24</td>\n","      <td>2.11</td>\n","      <td>0.25</td>\n","      <td>2.13</td>\n","      <td>2.06</td>\n","      <td>1.81</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2548</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>4.40</td>\n","      <td>7.75</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>1.37</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>4.333</td>\n","      <td>1.40</td>\n","      <td>4.20</td>\n","      <td>9.00</td>\n","      <td>4.50</td>\n","      <td>5.00</td>\n","      <td>9.00</td>\n","      <td>1.41</td>\n","      <td>1.40</td>\n","      <td>5.07</td>\n","      <td>4.55</td>\n","      <td>9.10</td>\n","      <td>8.04</td>\n","      <td>1.68</td>\n","      <td>1.63</td>\n","      <td>2.45</td>\n","      <td>2.23</td>\n","      <td>-1.25</td>\n","      <td>1.93</td>\n","      <td>1.90</td>\n","      <td>1.97</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2549</th>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.62</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>5.40</td>\n","      <td>4.50</td>\n","      <td>3.75</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>3.750</td>\n","      <td>5.00</td>\n","      <td>3.60</td>\n","      <td>1.70</td>\n","      <td>4.00</td>\n","      <td>4.20</td>\n","      <td>1.67</td>\n","      <td>5.60</td>\n","      <td>5.02</td>\n","      <td>4.28</td>\n","      <td>3.93</td>\n","      <td>1.70</td>\n","      <td>1.65</td>\n","      <td>1.67</td>\n","      <td>1.58</td>\n","      <td>2.50</td>\n","      <td>2.31</td>\n","      <td>0.75</td>\n","      <td>2.14</td>\n","      <td>2.08</td>\n","      <td>1.81</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>1.60</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>5.75</td>\n","      <td>1.60</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>6.00</td>\n","      <td>1.55</td>\n","      <td>4.000</td>\n","      <td>1.62</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>4.00</td>\n","      <td>4.30</td>\n","      <td>6.00</td>\n","      <td>1.62</td>\n","      <td>1.57</td>\n","      <td>4.35</td>\n","      <td>3.95</td>\n","      <td>6.40</td>\n","      <td>5.80</td>\n","      <td>1.67</td>\n","      <td>1.59</td>\n","      <td>2.43</td>\n","      <td>2.26</td>\n","      <td>-1.00</td>\n","      <td>2.01</td>\n","      <td>1.96</td>\n","      <td>1.90</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2551 rows × 63 columns</p>\n","</div>"],"text/plain":["      FTAG  HTHG  HTAG  D  E  F   HST    AF  ...  b6  c6  b7  c7  b8  c8  b9  c9\n","0      2.0   2.0   2.0  0  0  1   2.0  16.0  ...   1   0   1   0   1   0   1   0\n","1      2.0   0.0   1.0  0  1  0   5.0  14.0  ...   0   1   0   1   0   1   0   1\n","2      0.0   0.0   0.0  0  0  1   7.0  13.0  ...   0   1   0   1   0   1   0   1\n","3      0.0   0.0   0.0  0  0  1   8.0  11.0  ...   0   0   0   0   0   0   0   0\n","4      0.0   0.0   0.0  0  0  1   2.0  11.0  ...   0   1   0   1   0   1   0   1\n","...    ...   ...   ... .. .. ..   ...   ...  ...  ..  ..  ..  ..  ..  ..  ..  ..\n","2546   1.0   0.0   1.0  0  1  0   5.0  11.0  ...   1   0   1   0   1   0   1   0\n","2547   0.0   0.0   0.0  0  0  1   8.0   5.0  ...   0   0   0   0   0   0   0   0\n","2548   0.0   1.0   0.0  1  0  0   9.0  12.0  ...   0   0   0   0   0   0   0   0\n","2549   3.0   2.0   2.0  0  0  1   8.0  10.0  ...   0   1   0   1   0   1   0   1\n","2550   2.0   2.0   1.0  1  0  0  10.0   9.0  ...   0   1   0   1   0   1   0   1\n","\n","[2551 rows x 63 columns]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"gvNDz9CX7ZQQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1594335006072,"user_tz":240,"elapsed":335,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"a0169b83-ba2a-43b7-ae5e-bf2a5d0f0ed7"},"source":["x = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD','a0', 'a1', 'a2', 'a3', 'a4'],axis = 1)\n","x = x.iloc[:, 1:]\n","y = model_data.A\n","x1 = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD', 'b0', 'b1', 'b2' ,'b3', 'b4'],axis = 1)\n","x1 = x.iloc[:, 1:]\n","y1 = model_data.B\n","logreg_model = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logreg_model_1 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","rfecv = RFECV(estimator=logreg_model, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n","rfecv_1 = RFECV(estimator=logreg_model_1, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n","rfecv.fit(x, y)\n","rfecv_1.fit(x1, y1)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-06268b4b9e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FTHG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B365A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SJA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bb1X2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BbOU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BbAH'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'BbMxAHA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GBD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SJH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SBA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VCH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IWA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IWD'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FTHG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B365A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SJA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bb1X2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BbOU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BbAH'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'BbMxAHA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GBD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SJH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SBA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VCH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IWA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IWD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b2'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'b3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m         )\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['a0' 'a1' 'a2' 'a3' 'a4'] not found in axis\""]}]},{"cell_type":"code","metadata":{"id":"D0s4-0HDs4AA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594333836252,"user_tz":240,"elapsed":418,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"fe325db6-ad88-4a1b-bc56-e67759a75c9d"},"source":["print('Optimal number of features: {}'.format(rfecv.n_features_))\n","print('Optimal number of features: {}'.format(rfecv_1.n_features_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Optimal number of features: 41\n","Optimal number of features: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__B15qt67fiK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1594331333718,"user_tz":240,"elapsed":468,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"fa223d7d-dbb3-453d-e7e0-d0f684b9a6f8"},"source":["print(rfecv.support_)\n","print(rfecv.ranking_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ True  True  True  True  True  True  True False  True  True  True  True\n","  True False  True  True  True  True  True False False False  True  True\n","  True  True  True False  True False  True  True  True  True False  True\n"," False  True  True  True  True  True False  True  True False  True  True\n","  True  True  True  True False]\n","[ 1  1  1  1  1  1  1 12  1  1  1  1  1 13  1  1  1  1  1  3  4  7  1  1\n","  1  1  1 10  1  9  1  1  1  1  6  1 11  1  1  1  1  1  2  1  1  5  1  1\n","  1  1  1  1  8]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_rSlZlPUIz3K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1594331344801,"user_tz":240,"elapsed":360,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"75d4eefd-a954-4282-8d6a-980c7b0d60ef"},"source":["print(rfecv_1.support_)\n","print(rfecv_1.ranking_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ True  True False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False]\n","[ 1  1 28  9 41 47 50  3  2 48 26  8  5 21  7 31 36 29 10 51 25 27 30 43\n"," 45 44 42 14 34 35 12 40 46 13  6 11 33  4 37 23 22 38 15 49 17 19 24 32\n"," 18 20 16 39]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tJulbjLJJK9k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":959},"executionInfo":{"status":"ok","timestamp":1594334024486,"user_tz":240,"elapsed":523,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"1f1ced86-73bd-4272-87db-3a80184b2eb8"},"source":["FF = list(x.columns.values.tolist())\n","for i in range(len(FF)):\n","  print(i,end=\" \")\n","  print(FF[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 FTAG\n","1 HTHG\n","2 HTAG\n","3 D\n","4 E\n","5 F\n","6 HST\n","7 AF\n","8 HR\n","9 AR\n","10 B365H\n","11 B365D\n","12 BWH\n","13 BWD\n","14 BWA\n","15 GBH\n","16 GBA\n","17 IWH\n","18 LBH\n","19 LBD\n","20 LBA\n","21 SBH\n","22 SBD\n","23 WHH\n","24 WHD\n","25 WHA\n","26 SJD\n","27 VCD\n","28 VCA\n","29 BbMxH\n","30 BbAvH\n","31 BbMxD\n","32 BbAvD\n","33 BbMxA\n","34 BbAvA\n","35 BbMx>2.5\n","36 BbAv>2.5\n","37 BbMx<2.5\n","38 BbAv<2.5\n","39 BbAHh\n","40 BbMxAHH\n","41 BbAvAHH\n","42 BbAvAHA\n","43 b0\n","44 c0\n","45 b1\n","46 c1\n","47 b2\n","48 c2\n","49 b3\n","50 c3\n","51 b4\n","52 c4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uAB_c1ppt3cj","colab_type":"code","colab":{}},"source":["\n","x = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD','AF', 'BWD', 'LBD', 'LBA', 'SBH', 'VCD', 'VCA', 'BbMxH', 'BbAvA', 'BbAv>2.5', 'BbAvAHA'],axis = 1)\n","#x =x.drop(x.columns[[0, 33, 36, 39, 42, 45]], axis = 1) \n","y = model_data.A\n","\n","x1 = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD','AF', 'BWD', 'LBD', 'LBA', 'SBH', 'VCD', 'VCA', 'BbMxH', 'BbAvA', 'BbAv>2.5', 'BbAvAHA'],axis = 1)\n","x1 =x1.drop(x1.columns[[0,34,37,40,43,46]], axis = 1) \n","y1 = model_data.B\n","\n","x2 = model_data.drop(['A', 'B', 'C','FTHG', 'HF', 'AC', 'B365A', 'SJA', 'Bb1X2', 'BbOU', 'BbAH' , 'BbMxAHA', 'AST', 'AS', 'GBD', 'HS', 'SJH', 'HY', 'SBA', 'AY', 'VCH', 'HC', 'IWA', 'IWD','AF', 'BWD', 'LBD', 'LBA', 'SBH', 'VCD', 'VCA', 'BbMxH', 'BbAvA', 'BbAv>2.5', 'BbAvAHA'],axis = 1)\n","x2 = x2.drop(x2.columns[[0,35,38,41,44,47]], axis = 1) \n","y2 = model_data.C"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdMP3hrH4JYV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"status":"ok","timestamp":1594338551750,"user_tz":240,"elapsed":478,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"62863d1e-1ab4-4a73-c262-c334de3fdd1b"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>FTAG</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>HST</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>BWH</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>LBH</th>\n","      <th>SBD</th>\n","      <th>WHH</th>\n","      <th>WHD</th>\n","      <th>WHA</th>\n","      <th>SJD</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>a0</th>\n","      <th>b0</th>\n","      <th>c0</th>\n","      <th>a1</th>\n","      <th>b1</th>\n","      <th>c1</th>\n","      <th>a2</th>\n","      <th>b2</th>\n","      <th>c2</th>\n","      <th>a3</th>\n","      <th>b3</th>\n","      <th>c3</th>\n","      <th>a4</th>\n","      <th>b4</th>\n","      <th>c4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>2.10</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>2.10</td>\n","      <td>3.200</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>3.25</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>2.20</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>4.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>1.83</td>\n","      <td>3.80</td>\n","      <td>3.75</td>\n","      <td>3.600</td>\n","      <td>4.33</td>\n","      <td>3.20</td>\n","      <td>1.72</td>\n","      <td>3.25</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>2.10</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.30</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>2.95</td>\n","      <td>2.20</td>\n","      <td>2.25</td>\n","      <td>3.200</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.62</td>\n","      <td>3.20</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.24</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>1.65</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>4.75</td>\n","      <td>1.70</td>\n","      <td>1.67</td>\n","      <td>3.400</td>\n","      <td>1.70</td>\n","      <td>3.30</td>\n","      <td>4.33</td>\n","      <td>3.25</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>2.10</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.90</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>2.45</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.200</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.23</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2546</th>\n","      <td>375</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>6.75</td>\n","      <td>1.42</td>\n","      <td>7.00</td>\n","      <td>1.40</td>\n","      <td>6.00</td>\n","      <td>7.50</td>\n","      <td>4.600</td>\n","      <td>7.00</td>\n","      <td>4.33</td>\n","      <td>1.44</td>\n","      <td>4.80</td>\n","      <td>7.23</td>\n","      <td>4.90</td>\n","      <td>4.51</td>\n","      <td>1.47</td>\n","      <td>1.67</td>\n","      <td>2.43</td>\n","      <td>2.27</td>\n","      <td>1.25</td>\n","      <td>1.95</td>\n","      <td>1.89</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2547</th>\n","      <td>376</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.40</td>\n","      <td>3.50</td>\n","      <td>3.30</td>\n","      <td>2.05</td>\n","      <td>3.25</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.40</td>\n","      <td>3.500</td>\n","      <td>3.40</td>\n","      <td>3.30</td>\n","      <td>2.15</td>\n","      <td>3.50</td>\n","      <td>3.38</td>\n","      <td>3.72</td>\n","      <td>3.45</td>\n","      <td>2.20</td>\n","      <td>1.77</td>\n","      <td>2.24</td>\n","      <td>2.11</td>\n","      <td>0.25</td>\n","      <td>2.13</td>\n","      <td>2.06</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2548</th>\n","      <td>377</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>7.75</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>1.37</td>\n","      <td>1.40</td>\n","      <td>4.333</td>\n","      <td>1.40</td>\n","      <td>4.20</td>\n","      <td>9.00</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>5.07</td>\n","      <td>4.55</td>\n","      <td>9.10</td>\n","      <td>1.68</td>\n","      <td>2.45</td>\n","      <td>2.23</td>\n","      <td>-1.25</td>\n","      <td>1.93</td>\n","      <td>1.90</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2549</th>\n","      <td>378</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>5.00</td>\n","      <td>1.62</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>5.40</td>\n","      <td>4.50</td>\n","      <td>3.750</td>\n","      <td>5.00</td>\n","      <td>3.60</td>\n","      <td>1.70</td>\n","      <td>4.00</td>\n","      <td>5.02</td>\n","      <td>4.28</td>\n","      <td>3.93</td>\n","      <td>1.70</td>\n","      <td>1.67</td>\n","      <td>2.50</td>\n","      <td>2.31</td>\n","      <td>0.75</td>\n","      <td>2.14</td>\n","      <td>2.08</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>379</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>1.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>5.75</td>\n","      <td>1.60</td>\n","      <td>1.57</td>\n","      <td>4.000</td>\n","      <td>1.62</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>4.00</td>\n","      <td>1.57</td>\n","      <td>4.35</td>\n","      <td>3.95</td>\n","      <td>6.40</td>\n","      <td>1.67</td>\n","      <td>2.43</td>\n","      <td>2.26</td>\n","      <td>-1.00</td>\n","      <td>2.01</td>\n","      <td>1.96</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2551 rows × 48 columns</p>\n","</div>"],"text/plain":["      Unnamed: 0  FTAG  HTHG  HTAG  D  E  F  ...  c2  a3  b3  c3  a4  b4  c4\n","0              0   2.0   2.0   2.0  0  0  1  ...   0   0   1   0   0   1   0\n","1              1   2.0   0.0   1.0  0  1  0  ...   1   1   0   1   1   0   1\n","2              2   0.0   0.0   0.0  0  0  1  ...   1   0   0   1   0   0   1\n","3              3   0.0   0.0   0.0  0  0  1  ...   0   1   0   0   1   0   0\n","4              4   0.0   0.0   0.0  0  0  1  ...   1   0   0   1   0   0   1\n","...          ...   ...   ...   ... .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..\n","2546         375   1.0   0.0   1.0  0  1  0  ...   0   0   1   0   0   1   0\n","2547         376   0.0   0.0   0.0  0  0  1  ...   0   1   0   0   1   0   0\n","2548         377   0.0   1.0   0.0  1  0  0  ...   0   1   0   0   1   0   0\n","2549         378   3.0   2.0   2.0  0  0  1  ...   1   1   0   1   1   0   1\n","2550         379   2.0   2.0   1.0  1  0  0  ...   1   1   0   1   1   0   1\n","\n","[2551 rows x 48 columns]"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"Ab46SqZF13dS","colab_type":"code","colab":{}},"source":["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)\n","logistic_regression = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logistic_regression.fit(x_train,y_train)\n","y_pred = logistic_regression.predict(x_test)\n","y_train_predict = logistic_regression.predict(x_train)\n","\n","x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.3, random_state=4)\n","logistic_regression1 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logistic_regression1.fit(x1_train,y1_train)\n","y1_pred = logistic_regression1.predict(x1_test)\n","y1_train_predict = logistic_regression1.predict(x1_train)\n","\n","x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.3, random_state=4)\n","logistic_regression2 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","logistic_regression2.fit(x2_train,y2_train)\n","y2_pred = logistic_regression2.predict(x2_test)\n","y2_train_predict = logistic_regression2.predict(x2_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ER0DHLks2yB9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1594338763523,"user_tz":240,"elapsed":417,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"780f2739-5ad2-4604-ed73-f05f47390f1d"},"source":["accuracy_test = metrics.accuracy_score(y_test, y_pred)\n","accuracy_percentage_test = 100 * accuracy_test\n","print (\"Test data accuracy for model A: \", accuracy_percentage_test)\n","accuracy_train = metrics.accuracy_score(y_train, y_train_predict)\n","accuracy_percentage_train = 100 * accuracy_train\n","print (\"Train data Accuracy for model A:\",accuracy_percentage_train)\n","print (\" \")\n","accuracy_test_1 = metrics.accuracy_score(y1_test, y1_pred)\n","accuracy_percentage_test_1 = 100 * accuracy_test_1\n","print (\"Test data accuracy for model B: \", accuracy_percentage_test_1)\n","accuracy_train_1 = metrics.accuracy_score(y1_train, y1_train_predict)\n","accuracy_percentage_train_1 = 100 * accuracy_train_1\n","print (\"Train data Accuracy for model B:\",accuracy_percentage_train_1)\n","print (\" \")   \n","accuracy_test_2 = metrics.accuracy_score(y2_test, y2_pred)\n","accuracy_percentage_test_2 = 100 * accuracy_test_2\n","print (\"Test data accuracy for model C: \",accuracy_percentage_test_2)\n","accuracy_train_2 = metrics.accuracy_score(y2_train, y2_train_predict)\n","accuracy_percentage_train_2 = 100 * accuracy_train_2\n","print (\"Train data Accuracy for model C:\",accuracy_percentage_train_2)\n","print(\"  \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test data accuracy for model A:  81.20104438642298\n","Train data Accuracy for model A: 82.68907563025209\n"," \n","Test data accuracy for model B:  87.0757180156658\n","Train data Accuracy for model B: 88.8515406162465\n"," \n","Test data accuracy for model C:  65.79634464751958\n","Train data Accuracy for model C: 64.48179271708683\n","  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I7P0unpY3Nlh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594778703216,"user_tz":240,"elapsed":70101,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"622de1dc-ef38-4429-ed4a-069d36937d64"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","df = pd.DataFrame()\n","df1 = pd.DataFrame()\n","df2 = pd.DataFrame()\n","model_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Summer Research/Data/model_data.csv')\n","x = model_data.drop(['A', 'B', 'C','FTHG'],axis = 1)\n","x = x.iloc[:, 1:]\n","y = model_data.A\n","x1 = model_data.drop(['A', 'B', 'C','FTHG'],axis = 1)\n","x1 = x1.iloc[:, 1:]\n","y1 = model_data.B\n","x2 = model_data.drop(['A', 'B', 'C','FTHG'],axis = 1)\n","x2 = x2.iloc[:, 1:]\n","y2 = model_data.C\n","\n","col_lst = ['FTAG', 'HTHG','HTAG', 'D', 'E', 'F', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR', 'B365H', 'B365D', 'B365A', 'BWH',\n","           \t'BWD', 'BWA', 'GBH', 'GBD', 'GBA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'SBH', 'SBD', 'SBA', 'WHH', 'WHD', 'WHA', 'SJH', 'SJD', 'SJA', 'VCH',\n","           'VCD', 'VCA', 'Bb1X2', 'BbMxH', 'BbAvH', 'BbMxD', 'BbAvD', 'BbMxA', 'BbAvA', 'BbOU', 'BbMx>2.5', 'BbAv>2.5', 'BbMx<2.5', 'BbAv<2.5', 'BbAH', 'BbAHh',\n","           'BbMxAHH', 'BbAvAHH', 'BbMxAHA', 'BbAvAHA']\n","for i in col_lst:\n","    # For model A\n","    y = model_data.A\n","    #removing the column\n","    x9 = x.drop(i, axis=1)\n","    print(\"x9 after droping column is:\")\n","    print(\"\",x9)\n","    print(\" \")\n","    #logistic regression\n","    x_train, x_test, y_train, y_test = train_test_split(x9, y, test_size=0.3, random_state=4)\n","    logistic_regression = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","    logistic_regression.fit(x_train,y_train)\n","    y_pred = logistic_regression.predict(x_test)\n","    y_train_predict = logistic_regression.predict(x_train)\n","    print(\"the test prediction looks like this:\",y_pred)\n","    print(\" \")\n","    print(\"the train prediction looks like this:\",y_train_predict)\n","    print(\" \")\n","    df[i]=np.append(y_pred, y_train_predict)\n","    print(\" Adding test and Train prediction in a empty dataset:\")\n","    print(\"\",df)\n","    print(\" \")\n","    #adding the column back for next iteration\n","    x9[i]=x[i]\n","\n","    #for model B\n","    y1 = model_data.B\n","    #removing the column\n","    x19 = x1.drop(i, axis=1)\n","    print(\"x19 after droping the column is:\")\n","    print(\"\",x19)\n","    print(\" \")\n","    #logistic regression\n","    x1_train, x1_test, y1_train, y1_test = train_test_split(x19, y1, test_size=0.3, random_state=4)\n","    logistic_regression1 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","    logistic_regression1.fit(x1_train,y1_train)\n","    y1_pred = logistic_regression1.predict(x1_test)\n","    y1_train_predict = logistic_regression1.predict(x1_train)\n","    print(\"the test prediction looks like this:\",y1_pred)\n","    print(\" \")\n","    print(\"the train prediction looks like this:\",y1_train_predict)\n","    print(\" \")\n","    df1[i]=np.append(y1_pred, y1_train_predict)\n","    print(\" Adding test and Train prediction in a empty dataset:\")\n","    print(\"\",df1)\n","    print(\" \")\n","    # Addinng the column back for next iteration\n","    x19[i]=x1[i]\n","\n","    #for model C\n","    y2 = model_data.C\n","    #removing the column\n","    x29 = x2.drop(i, axis=1)\n","    print(\"x29 after droping the column is:\")\n","    print(\"\",x29)\n","    print(\" \")\n","    #logistic regression\n","    x2_train, x2_test, y2_train, y2_test = train_test_split(x29, y2, test_size=0.3, random_state=4)\n","    logistic_regression2 = LogisticRegression(multi_class = \"ovr\", solver = 'newton-cg', class_weight = 'balanced')\n","    logistic_regression2.fit(x2_train,y2_train)\n","    y2_pred = logistic_regression2.predict(x2_test)\n","    y2_train_predict = logistic_regression2.predict(x2_train)\n","    print(\"the test prediction looks like this:\",y2_pred)\n","    print(\" \")\n","    print(\"the train prediction looks like this:\",y2_train_predict)\n","    print(\" \")\n","    df2[i]=np.append(y2_pred, y2_train_predict)\n","    print(\" Adding test and Train prediction in a empty dataset:\")\n","    print(\"\",df2)\n","    print(\" \")\n","    #adding the column back for next iteration\n","    x29[i]=x2[i]\n","    print(\"end of cycle:\",i)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  GBA  IWH  IWD  IWA  LBH  LBD  LBA  SBH\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 34 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0\n"," 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  GBA  IWH  IWD  IWA  LBH  LBD  LBA  SBH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 34 columns]\n"," \n","end of cycle: SBH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWH  IWD  IWA  LBH  LBD  LBA  SBH  SBD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 35 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWH  IWD  IWA  LBH  LBD  LBA  SBH  SBD\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 35 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWH  IWD  IWA  LBH  LBD  LBA  SBH  SBD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 35 columns]\n"," \n","end of cycle: SBD\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWD  IWA  LBH  LBD  LBA  SBH  SBD  SBA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 36 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWD  IWA  LBH  LBD  LBA  SBH  SBD  SBA\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    0    0    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 36 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWD  IWA  LBH  LBD  LBA  SBH  SBD  SBA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 36 columns]\n"," \n","end of cycle: SBA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWA  LBH  LBD  LBA  SBH  SBD  SBA  WHH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 37 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWA  LBH  LBD  LBA  SBH  SBD  SBA  WHH\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    0    1    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 37 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  IWA  LBH  LBD  LBA  SBH  SBD  SBA  WHH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 37 columns]\n"," \n","end of cycle: WHH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBH  LBD  LBA  SBH  SBD  SBA  WHH  WHD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 38 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBH  LBD  LBA  SBH  SBD  SBA  WHH  WHD\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    1    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 38 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBH  LBD  LBA  SBH  SBD  SBA  WHH  WHD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 38 columns]\n"," \n","end of cycle: WHD\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBD  LBA  SBH  SBD  SBA  WHH  WHD  WHA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 39 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBD  LBA  SBH  SBD  SBA  WHH  WHD  WHA\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    1    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 39 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBD  LBA  SBH  SBD  SBA  WHH  WHD  WHA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 39 columns]\n"," \n","end of cycle: WHA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBA  SBH  SBD  SBA  WHH  WHD  WHA  SJH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 40 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBA  SBH  SBD  SBA  WHH  WHD  WHA  SJH\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    1    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 40 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  LBA  SBH  SBD  SBA  WHH  WHD  WHA  SJH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 40 columns]\n"," \n","end of cycle: SJH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBH  SBD  SBA  WHH  WHD  WHA  SJH  SJD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 41 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBH  SBD  SBA  WHH  WHD  WHA  SJH  SJD\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    1    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 41 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBH  SBD  SBA  WHH  WHD  WHA  SJH  SJD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 41 columns]\n"," \n","end of cycle: SJD\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBD  SBA  WHH  WHD  WHA  SJH  SJD  SJA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 42 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBD  SBA  WHH  WHD  WHA  SJH  SJD  SJA\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    1    0    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 42 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBD  SBA  WHH  WHD  WHA  SJH  SJD  SJA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 42 columns]\n"," \n","end of cycle: SJA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBA  WHH  WHD  WHA  SJH  SJD  SJA  VCH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 43 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBA  WHH  WHD  WHA  SJH  SJD  SJA  VCH\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    1    0    0    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 43 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  SBA  WHH  WHD  WHA  SJH  SJD  SJA  VCH\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 43 columns]\n"," \n","end of cycle: VCH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  WHH  WHD  WHA  SJH  SJD  SJA  VCH  VCD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 44 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  WHH  WHD  WHA  SJH  SJD  SJA  VCH  VCD\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 44 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  WHH  WHD  WHA  SJH  SJD  SJA  VCH  VCD\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 44 columns]\n"," \n","end of cycle: VCD\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  WHD  WHA  SJH  SJD  SJA  VCH  VCD  VCA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        0     1     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    1\n","2        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","3        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2548     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 45 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  WHD  WHA  SJH  SJD  SJA  VCH  VCD  VCA\n","0        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","1        1     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2        0     1     0  0  0  0   0   1  ...    0    0    0    0    0    0    0    0\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2550     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","\n","[2551 rows x 45 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  AS  ...  WHD  WHA  SJH  SJD  SJA  VCH  VCD  VCA\n","0        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","1        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","3        0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","4        1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","...    ...   ...   ... .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n","2546     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2547     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2548     0     0     0  0  0  0   0   0  ...    0    0    0    0    0    0    0    0\n","2549     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","2550     1     1     1  1  1  1   1   1  ...    1    1    1    1    1    1    1    1\n","\n","[2551 rows x 45 columns]\n"," \n","end of cycle: VCA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJH  SJD  SJA  VCH  VCD  VCA  Bb1X2\n","0        0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","1        0     1     0  0  0  0   0  ...    0    0    0    0    0    1      0\n","2        1     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","3        1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","4        0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...  ...  ...    ...\n","2546     1     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2547     1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","2548     1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","2549     1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","\n","[2551 rows x 46 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJH  SJD  SJA  VCH  VCD  VCA  Bb1X2\n","0        1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","1        1     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2        0     1     0  0  0  0   0  ...    0    0    0    0    0    0      1\n","3        0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","4        0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...  ...  ...    ...\n","2546     0     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","2547     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2549     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2550     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","\n","[2551 rows x 46 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1\n"," 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n"," 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n"," 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJH  SJD  SJA  VCH  VCD  VCA  Bb1X2\n","0        0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","1        1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","2        1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","3        0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","4        1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...  ...  ...    ...\n","2546     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2547     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0    0    0    0      0\n","2549     1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1    1    1    1      1\n","\n","[2551 rows x 46 columns]\n"," \n","end of cycle: Bb1X2\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJD  SJA  VCH  VCD  VCA  Bb1X2  BbMxH\n","0        0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","1        0     1     0  0  0  0   0  ...    0    0    0    0    1      0      0\n","2        1     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","3        1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","4        0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...  ...    ...    ...\n","2546     1     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2547     1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","2548     1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","2549     1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","\n","[2551 rows x 47 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJD  SJA  VCH  VCD  VCA  Bb1X2  BbMxH\n","0        1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","1        1     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2        0     1     0  0  0  0   0  ...    0    0    0    0    0      1      0\n","3        0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","4        0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...  ...    ...    ...\n","2546     0     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","2547     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2549     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2550     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","\n","[2551 rows x 47 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJD  SJA  VCH  VCD  VCA  Bb1X2  BbMxH\n","0        0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","1        1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","2        1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","3        0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","4        1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...  ...    ...    ...\n","2546     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2547     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0    0    0      0      0\n","2549     1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1    1    1      1      1\n","\n","[2551 rows x 47 columns]\n"," \n","end of cycle: BbMxH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJA  VCH  VCD  VCA  Bb1X2  BbMxH  BbAvH\n","0        0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","1        0     1     0  0  0  0   0  ...    0    0    0    1      0      0      0\n","2        1     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","3        1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","4        0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...    ...    ...    ...\n","2546     1     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2547     1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","2548     1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","2549     1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","\n","[2551 rows x 48 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJA  VCH  VCD  VCA  Bb1X2  BbMxH  BbAvH\n","0        1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","1        1     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2        0     1     0  0  0  0   0  ...    0    0    0    0      1      0      0\n","3        0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","4        0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...    ...    ...    ...\n","2546     0     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","2547     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2549     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2550     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","\n","[2551 rows x 48 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  SJA  VCH  VCD  VCA  Bb1X2  BbMxH  BbAvH\n","0        0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","1        1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","2        1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","3        0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","4        1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...  ...    ...    ...    ...\n","2546     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2547     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0    0      0      0      0\n","2549     1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1    1      1      1      1\n","\n","[2551 rows x 48 columns]\n"," \n","end of cycle: BbAvH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  VCH  VCD  VCA  Bb1X2  BbMxH  BbAvH  BbMxD\n","0        0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","1        0     1     0  0  0  0   0  ...    0    0    1      0      0      0      0\n","2        1     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","3        1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","4        0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...    ...    ...    ...    ...\n","2546     1     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2547     1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","2548     1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","2549     1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","\n","[2551 rows x 49 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  VCH  VCD  VCA  Bb1X2  BbMxH  BbAvH  BbMxD\n","0        1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","1        1     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2        0     1     0  0  0  0   0  ...    0    0    0      1      0      0      0\n","3        0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","4        0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...    ...    ...    ...    ...\n","2546     0     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","2547     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2549     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2550     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","\n","[2551 rows x 49 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  HS  ...  VCH  VCD  VCA  Bb1X2  BbMxH  BbAvH  BbMxD\n","0        0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","1        1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","2        1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","3        0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","4        1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","...    ...   ...   ... .. .. ..  ..  ...  ...  ...  ...    ...    ...    ...    ...\n","2546     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2547     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2548     0     0     0  0  0  0   0  ...    0    0    0      0      0      0      0\n","2549     1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","2550     1     1     1  1  1  1   1  ...    1    1    1      1      1      1      1\n","\n","[2551 rows x 49 columns]\n"," \n","end of cycle: BbMxD\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  VCA  Bb1X2  BbMxH  BbAvH  BbMxD  BbAvD\n","0        0     0     0  0  0  0  ...    0      0      0      0      0      0\n","1        0     1     0  0  0  0  ...    1      0      0      0      0      0\n","2        1     0     0  0  0  0  ...    0      0      0      0      0      0\n","3        1     1     1  1  1  1  ...    1      1      1      1      1      1\n","4        0     0     0  0  0  0  ...    0      0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ...  ...    ...    ...    ...    ...    ...\n","2546     1     0     0  0  0  0  ...    0      0      0      0      0      0\n","2547     1     1     1  1  1  1  ...    1      1      1      1      1      1\n","2548     1     1     1  1  1  1  ...    1      1      1      1      1      1\n","2549     1     1     1  1  1  1  ...    1      1      1      1      1      1\n","2550     1     1     1  1  1  1  ...    1      1      1      1      1      1\n","\n","[2551 rows x 50 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  VCA  Bb1X2  BbMxH  BbAvH  BbMxD  BbAvD\n","0        1     1     1  1  1  1  ...    1      1      1      1      1      1\n","1        1     0     0  0  0  0  ...    0      0      0      0      0      0\n","2        0     1     0  0  0  0  ...    0      1      0      0      0      0\n","3        0     0     0  0  0  0  ...    0      0      0      0      0      0\n","4        0     0     0  0  0  0  ...    0      0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ...  ...    ...    ...    ...    ...    ...\n","2546     0     1     1  1  1  1  ...    1      1      1      1      1      1\n","2547     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","2548     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","2549     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","2550     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","\n","[2551 rows x 50 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  VCA  Bb1X2  BbMxH  BbAvH  BbMxD  BbAvD\n","0        0     0     0  0  0  0  ...    0      0      0      0      0      0\n","1        1     1     1  1  1  1  ...    1      1      1      1      1      1\n","2        1     1     1  1  1  1  ...    1      1      1      1      1      1\n","3        0     0     0  0  0  0  ...    0      0      0      0      0      0\n","4        1     1     1  1  1  1  ...    1      1      1      1      1      1\n","...    ...   ...   ... .. .. ..  ...  ...    ...    ...    ...    ...    ...\n","2546     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","2547     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","2548     0     0     0  0  0  0  ...    0      0      0      0      0      0\n","2549     1     1     1  1  1  1  ...    1      1      1      1      1      1\n","2550     1     1     1  1  1  1  ...    1      1      1      1      1      1\n","\n","[2551 rows x 50 columns]\n"," \n","end of cycle: BbAvD\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  Bb1X2  BbMxH  BbAvH  BbMxD  BbAvD  BbMxA\n","0        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","1        0     1     0  0  0  0  ...      0      0      0      0      0      0\n","2        1     0     0  0  0  0  ...      0      0      0      0      0      0\n","3        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","4        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...    ...\n","2546     1     0     0  0  0  0  ...      0      0      0      0      0      0\n","2547     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2548     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2549     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2550     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","\n","[2551 rows x 51 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  Bb1X2  BbMxH  BbAvH  BbMxD  BbAvD  BbMxA\n","0        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","1        1     0     0  0  0  0  ...      0      0      0      0      0      0\n","2        0     1     0  0  0  0  ...      1      0      0      0      0      1\n","3        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","4        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...    ...\n","2546     0     1     1  1  1  1  ...      1      1      1      1      1      1\n","2547     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2548     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2549     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2550     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","\n","[2551 rows x 51 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  Bb1X2  BbMxH  BbAvH  BbMxD  BbAvD  BbMxA\n","0        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","1        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","3        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","4        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...    ...\n","2546     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2547     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2548     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2549     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2550     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","\n","[2551 rows x 51 columns]\n"," \n","end of cycle: BbMxA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbMxH  BbAvH  BbMxD  BbAvD  BbMxA  BbAvA\n","0        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","1        0     1     0  0  0  0  ...      0      0      0      0      0      0\n","2        1     0     0  0  0  0  ...      0      0      0      0      0      0\n","3        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","4        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...    ...\n","2546     1     0     0  0  0  0  ...      0      0      0      0      0      0\n","2547     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2548     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2549     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2550     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","\n","[2551 rows x 52 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbMxH  BbAvH  BbMxD  BbAvD  BbMxA  BbAvA\n","0        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","1        1     0     0  0  0  0  ...      0      0      0      0      0      0\n","2        0     1     0  0  0  0  ...      0      0      0      0      1      0\n","3        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","4        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...    ...\n","2546     0     1     1  1  1  1  ...      1      1      1      1      1      1\n","2547     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2548     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2549     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2550     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","\n","[2551 rows x 52 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbMxH  BbAvH  BbMxD  BbAvD  BbMxA  BbAvA\n","0        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","1        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","3        0     0     0  0  0  0  ...      0      0      0      0      0      0\n","4        1     1     1  1  1  1  ...      1      1      1      1      1      1\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...    ...\n","2546     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2547     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2548     0     0     0  0  0  0  ...      0      0      0      0      0      0\n","2549     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","2550     1     1     1  1  1  1  ...      1      1      1      1      1      1\n","\n","[2551 rows x 52 columns]\n"," \n","end of cycle: BbAvA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbAvH  BbMxD  BbAvD  BbMxA  BbAvA  BbOU\n","0        0     0     0  0  0  0  ...      0      0      0      0      0     0\n","1        0     1     0  0  0  0  ...      0      0      0      0      0     0\n","2        1     0     0  0  0  0  ...      0      0      0      0      0     0\n","3        1     1     1  1  1  1  ...      1      1      1      1      1     1\n","4        0     0     0  0  0  0  ...      0      0      0      0      0     0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...   ...\n","2546     1     0     0  0  0  0  ...      0      0      0      0      0     0\n","2547     1     1     1  1  1  1  ...      1      1      1      1      1     1\n","2548     1     1     1  1  1  1  ...      1      1      1      1      1     1\n","2549     1     1     1  1  1  1  ...      1      1      1      1      1     1\n","2550     1     1     1  1  1  1  ...      1      1      1      1      1     1\n","\n","[2551 rows x 53 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbAvH  BbMxD  BbAvD  BbMxA  BbAvA  BbOU\n","0        1     1     1  1  1  1  ...      1      1      1      1      1     1\n","1        1     0     0  0  0  0  ...      0      0      0      0      0     0\n","2        0     1     0  0  0  0  ...      0      0      0      1      0     1\n","3        0     0     0  0  0  0  ...      0      0      0      0      0     0\n","4        0     0     0  0  0  0  ...      0      0      0      0      0     0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...   ...\n","2546     0     1     1  1  1  1  ...      1      1      1      1      1     1\n","2547     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","2548     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","2549     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","2550     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","\n","[2551 rows x 53 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbAvH  BbMxD  BbAvD  BbMxA  BbAvA  BbOU\n","0        0     0     0  0  0  0  ...      0      0      0      0      0     0\n","1        1     1     1  1  1  1  ...      1      1      1      1      1     1\n","2        1     1     1  1  1  1  ...      1      1      1      1      1     1\n","3        0     0     0  0  0  0  ...      0      0      0      0      0     0\n","4        1     1     1  1  1  1  ...      1      1      1      1      1     1\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...    ...   ...\n","2546     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","2547     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","2548     0     0     0  0  0  0  ...      0      0      0      0      0     0\n","2549     1     1     1  1  1  1  ...      1      1      1      1      1     1\n","2550     1     1     1  1  1  1  ...      1      1      1      1      1     1\n","\n","[2551 rows x 53 columns]\n"," \n","end of cycle: BbOU\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbMxD  BbAvD  BbMxA  BbAvA  BbOU  BbMx>2.5\n","0        0     0     0  0  0  0  ...      0      0      0      0     0         0\n","1        0     1     0  0  0  0  ...      0      0      0      0     0         0\n","2        1     0     0  0  0  0  ...      0      0      0      0     0         0\n","3        1     1     1  1  1  1  ...      1      1      1      1     1         1\n","4        0     0     0  0  0  0  ...      0      0      0      0     0         0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...   ...       ...\n","2546     1     0     0  0  0  0  ...      0      0      0      0     0         0\n","2547     1     1     1  1  1  1  ...      1      1      1      1     1         1\n","2548     1     1     1  1  1  1  ...      1      1      1      1     1         1\n","2549     1     1     1  1  1  1  ...      1      1      1      1     1         1\n","2550     1     1     1  1  1  1  ...      1      1      1      1     1         1\n","\n","[2551 rows x 54 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbMxD  BbAvD  BbMxA  BbAvA  BbOU  BbMx>2.5\n","0        1     1     1  1  1  1  ...      1      1      1      1     1         1\n","1        1     0     0  0  0  0  ...      0      0      0      0     0         0\n","2        0     1     0  0  0  0  ...      0      0      1      0     1         0\n","3        0     0     0  0  0  0  ...      0      0      0      0     0         0\n","4        0     0     0  0  0  0  ...      0      0      0      0     0         0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...   ...       ...\n","2546     0     1     1  1  1  1  ...      1      1      1      1     1         1\n","2547     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","2548     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","2549     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","2550     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","\n","[2551 rows x 54 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbMxD  BbAvD  BbMxA  BbAvA  BbOU  BbMx>2.5\n","0        0     0     0  0  0  0  ...      0      0      0      0     0         0\n","1        1     1     1  1  1  1  ...      1      1      1      1     1         1\n","2        1     1     1  1  1  1  ...      1      1      1      1     1         1\n","3        0     0     0  0  0  0  ...      0      0      0      0     0         0\n","4        1     1     1  1  1  1  ...      1      1      1      1     1         1\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...    ...   ...       ...\n","2546     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","2547     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","2548     0     0     0  0  0  0  ...      0      0      0      0     0         0\n","2549     1     1     1  1  1  1  ...      1      1      1      1     1         1\n","2550     1     1     1  1  1  1  ...      1      1      1      1     1         1\n","\n","[2551 rows x 54 columns]\n"," \n","end of cycle: BbMx>2.5\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbAvD  BbMxA  BbAvA  BbOU  BbMx>2.5  BbAv>2.5\n","0        0     0     0  0  0  0  ...      0      0      0     0         0         0\n","1        0     1     0  0  0  0  ...      0      0      0     0         0         0\n","2        1     0     0  0  0  0  ...      0      0      0     0         0         0\n","3        1     1     1  1  1  1  ...      1      1      1     1         1         1\n","4        0     0     0  0  0  0  ...      0      0      0     0         0         0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...   ...       ...       ...\n","2546     1     0     0  0  0  0  ...      0      0      0     0         0         0\n","2547     1     1     1  1  1  1  ...      1      1      1     1         1         1\n","2548     1     1     1  1  1  1  ...      1      1      1     1         1         1\n","2549     1     1     1  1  1  1  ...      1      1      1     1         1         1\n","2550     1     1     1  1  1  1  ...      1      1      1     1         1         1\n","\n","[2551 rows x 55 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbAvD  BbMxA  BbAvA  BbOU  BbMx>2.5  BbAv>2.5\n","0        1     1     1  1  1  1  ...      1      1      1     1         1         1\n","1        1     0     0  0  0  0  ...      0      0      0     0         0         0\n","2        0     1     0  0  0  0  ...      0      1      0     1         0         0\n","3        0     0     0  0  0  0  ...      0      0      0     0         0         0\n","4        0     0     0  0  0  0  ...      0      0      0     0         0         0\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...   ...       ...       ...\n","2546     0     1     1  1  1  1  ...      1      1      1     1         1         1\n","2547     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","2548     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","2549     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","2550     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","\n","[2551 rows x 55 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  F  ...  BbAvD  BbMxA  BbAvA  BbOU  BbMx>2.5  BbAv>2.5\n","0        0     0     0  0  0  0  ...      0      0      0     0         0         0\n","1        1     1     1  1  1  1  ...      1      1      1     1         1         1\n","2        1     1     1  1  1  1  ...      1      1      1     1         1         1\n","3        0     0     0  0  0  0  ...      0      0      0     0         0         0\n","4        1     1     1  1  1  1  ...      1      1      1     1         1         1\n","...    ...   ...   ... .. .. ..  ...    ...    ...    ...   ...       ...       ...\n","2546     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","2547     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","2548     0     0     0  0  0  0  ...      0      0      0     0         0         0\n","2549     1     1     1  1  1  1  ...      1      1      1     1         1         1\n","2550     1     1     1  1  1  1  ...      1      1      1     1         1         1\n","\n","[2551 rows x 55 columns]\n"," \n","end of cycle: BbAv>2.5\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAvA  BbOU  BbMx>2.5  BbAv>2.5  BbMx<2.5\n","0        0     0     0  0  0  ...      0     0         0         0         0\n","1        0     1     0  0  0  ...      0     0         0         0         0\n","2        1     0     0  0  0  ...      0     0         0         0         0\n","3        1     1     1  1  1  ...      1     1         1         1         1\n","4        0     0     0  0  0  ...      0     0         0         0         0\n","...    ...   ...   ... .. ..  ...    ...   ...       ...       ...       ...\n","2546     1     0     0  0  0  ...      0     0         0         0         0\n","2547     1     1     1  1  1  ...      1     1         1         1         1\n","2548     1     1     1  1  1  ...      1     1         1         1         1\n","2549     1     1     1  1  1  ...      1     1         1         1         1\n","2550     1     1     1  1  1  ...      1     1         1         1         1\n","\n","[2551 rows x 56 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAvA  BbOU  BbMx>2.5  BbAv>2.5  BbMx<2.5\n","0        1     1     1  1  1  ...      1     1         1         1         1\n","1        1     0     0  0  0  ...      0     0         0         0         0\n","2        0     1     0  0  0  ...      0     1         0         0         0\n","3        0     0     0  0  0  ...      0     0         0         0         0\n","4        0     0     0  0  0  ...      0     0         0         0         0\n","...    ...   ...   ... .. ..  ...    ...   ...       ...       ...       ...\n","2546     0     1     1  1  1  ...      1     1         1         1         1\n","2547     0     0     0  0  0  ...      0     0         0         0         0\n","2548     0     0     0  0  0  ...      0     0         0         0         0\n","2549     0     0     0  0  0  ...      0     0         0         0         0\n","2550     0     0     0  0  0  ...      0     0         0         0         0\n","\n","[2551 rows x 56 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAvA  BbOU  BbMx>2.5  BbAv>2.5  BbMx<2.5\n","0        0     0     0  0  0  ...      0     0         0         0         0\n","1        1     1     1  1  1  ...      1     1         1         1         1\n","2        1     1     1  1  1  ...      1     1         1         1         1\n","3        0     0     0  0  0  ...      0     0         0         0         0\n","4        1     1     1  1  1  ...      1     1         1         1         1\n","...    ...   ...   ... .. ..  ...    ...   ...       ...       ...       ...\n","2546     0     0     0  0  0  ...      0     0         0         0         0\n","2547     0     0     0  0  0  ...      0     0         0         0         0\n","2548     0     0     0  0  0  ...      0     0         0         0         0\n","2549     1     1     1  1  1  ...      1     1         1         1         1\n","2550     1     1     1  1  1  ...      1     1         1         1         1\n","\n","[2551 rows x 56 columns]\n"," \n","end of cycle: BbMx<2.5\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbOU  BbMx>2.5  BbAv>2.5  BbMx<2.5  BbAv<2.5\n","0        0     0     0  0  0  ...     0         0         0         0         0\n","1        0     1     0  0  0  ...     0         0         0         0         0\n","2        1     0     0  0  0  ...     0         0         0         0         0\n","3        1     1     1  1  1  ...     1         1         1         1         1\n","4        0     0     0  0  0  ...     0         0         0         0         0\n","...    ...   ...   ... .. ..  ...   ...       ...       ...       ...       ...\n","2546     1     0     0  0  0  ...     0         0         0         0         0\n","2547     1     1     1  1  1  ...     1         1         1         1         1\n","2548     1     1     1  1  1  ...     1         1         1         1         1\n","2549     1     1     1  1  1  ...     1         1         1         1         1\n","2550     1     1     1  1  1  ...     1         1         1         1         1\n","\n","[2551 rows x 57 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbOU  BbMx>2.5  BbAv>2.5  BbMx<2.5  BbAv<2.5\n","0        1     1     1  1  1  ...     1         1         1         1         1\n","1        1     0     0  0  0  ...     0         0         0         0         0\n","2        0     1     0  0  0  ...     1         0         0         0         0\n","3        0     0     0  0  0  ...     0         0         0         0         0\n","4        0     0     0  0  0  ...     0         0         0         0         0\n","...    ...   ...   ... .. ..  ...   ...       ...       ...       ...       ...\n","2546     0     1     1  1  1  ...     1         1         1         1         1\n","2547     0     0     0  0  0  ...     0         0         0         0         0\n","2548     0     0     0  0  0  ...     0         0         0         0         0\n","2549     0     0     0  0  0  ...     0         0         0         0         0\n","2550     0     0     0  0  0  ...     0         0         0         0         0\n","\n","[2551 rows x 57 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbOU  BbMx>2.5  BbAv>2.5  BbMx<2.5  BbAv<2.5\n","0        0     0     0  0  0  ...     0         0         0         0         0\n","1        1     1     1  1  1  ...     1         1         1         1         1\n","2        1     1     1  1  1  ...     1         1         1         1         1\n","3        0     0     0  0  0  ...     0         0         0         0         0\n","4        1     1     1  1  1  ...     1         1         1         1         1\n","...    ...   ...   ... .. ..  ...   ...       ...       ...       ...       ...\n","2546     0     0     0  0  0  ...     0         0         0         0         0\n","2547     0     0     0  0  0  ...     0         0         0         0         0\n","2548     0     0     0  0  0  ...     0         0         0         0         0\n","2549     1     1     1  1  1  ...     1         1         1         1         1\n","2550     1     1     1  1  1  ...     1         1         1         1         1\n","\n","[2551 rows x 57 columns]\n"," \n","end of cycle: BbAv<2.5\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbMx>2.5  BbAv>2.5  BbMx<2.5  BbAv<2.5  BbAH\n","0        0     0     0  0  0  ...         0         0         0         0     0\n","1        0     1     0  0  0  ...         0         0         0         0     0\n","2        1     0     0  0  0  ...         0         0         0         0     0\n","3        1     1     1  1  1  ...         1         1         1         1     1\n","4        0     0     0  0  0  ...         0         0         0         0     0\n","...    ...   ...   ... .. ..  ...       ...       ...       ...       ...   ...\n","2546     1     0     0  0  0  ...         0         0         0         0     0\n","2547     1     1     1  1  1  ...         1         1         1         1     1\n","2548     1     1     1  1  1  ...         1         1         1         1     1\n","2549     1     1     1  1  1  ...         1         1         1         1     1\n","2550     1     1     1  1  1  ...         1         1         1         1     1\n","\n","[2551 rows x 58 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbMx>2.5  BbAv>2.5  BbMx<2.5  BbAv<2.5  BbAH\n","0        1     1     1  1  1  ...         1         1         1         1     1\n","1        1     0     0  0  0  ...         0         0         0         0     0\n","2        0     1     0  0  0  ...         0         0         0         0     0\n","3        0     0     0  0  0  ...         0         0         0         0     0\n","4        0     0     0  0  0  ...         0         0         0         0     0\n","...    ...   ...   ... .. ..  ...       ...       ...       ...       ...   ...\n","2546     0     1     1  1  1  ...         1         1         1         1     1\n","2547     0     0     0  0  0  ...         0         0         0         0     0\n","2548     0     0     0  0  0  ...         0         0         0         0     0\n","2549     0     0     0  0  0  ...         0         0         0         0     0\n","2550     0     0     0  0  0  ...         0         0         0         0     0\n","\n","[2551 rows x 58 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbMx>2.5  BbAv>2.5  BbMx<2.5  BbAv<2.5  BbAH\n","0        0     0     0  0  0  ...         0         0         0         0     0\n","1        1     1     1  1  1  ...         1         1         1         1     1\n","2        1     1     1  1  1  ...         1         1         1         1     1\n","3        0     0     0  0  0  ...         0         0         0         0     0\n","4        1     1     1  1  1  ...         1         1         1         1     1\n","...    ...   ...   ... .. ..  ...       ...       ...       ...       ...   ...\n","2546     0     0     0  0  0  ...         0         0         0         0     0\n","2547     0     0     0  0  0  ...         0         0         0         0     0\n","2548     0     0     0  0  0  ...         0         0         0         0     0\n","2549     1     1     1  1  1  ...         1         1         1         1     1\n","2550     1     1     1  1  1  ...         1         1         1         1     1\n","\n","[2551 rows x 58 columns]\n"," \n","end of cycle: BbAH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAv>2.5  BbMx<2.5  BbAv<2.5  BbAH  BbAHh\n","0        0     0     0  0  0  ...         0         0         0     0      0\n","1        0     1     0  0  0  ...         0         0         0     0      0\n","2        1     0     0  0  0  ...         0         0         0     0      0\n","3        1     1     1  1  1  ...         1         1         1     1      1\n","4        0     0     0  0  0  ...         0         0         0     0      0\n","...    ...   ...   ... .. ..  ...       ...       ...       ...   ...    ...\n","2546     1     0     0  0  0  ...         0         0         0     0      0\n","2547     1     1     1  1  1  ...         1         1         1     1      1\n","2548     1     1     1  1  1  ...         1         1         1     1      1\n","2549     1     1     1  1  1  ...         1         1         1     1      1\n","2550     1     1     1  1  1  ...         1         1         1     1      1\n","\n","[2551 rows x 59 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAv>2.5  BbMx<2.5  BbAv<2.5  BbAH  BbAHh\n","0        1     1     1  1  1  ...         1         1         1     1      1\n","1        1     0     0  0  0  ...         0         0         0     0      0\n","2        0     1     0  0  0  ...         0         0         0     0      0\n","3        0     0     0  0  0  ...         0         0         0     0      0\n","4        0     0     0  0  0  ...         0         0         0     0      0\n","...    ...   ...   ... .. ..  ...       ...       ...       ...   ...    ...\n","2546     0     1     1  1  1  ...         1         1         1     1      1\n","2547     0     0     0  0  0  ...         0         0         0     0      0\n","2548     0     0     0  0  0  ...         0         0         0     0      0\n","2549     0     0     0  0  0  ...         0         0         0     0      0\n","2550     0     0     0  0  0  ...         0         0         0     0      0\n","\n","[2551 rows x 59 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAv>2.5  BbMx<2.5  BbAv<2.5  BbAH  BbAHh\n","0        0     0     0  0  0  ...         0         0         0     0      0\n","1        1     1     1  1  1  ...         1         1         1     1      1\n","2        1     1     1  1  1  ...         1         1         1     1      1\n","3        0     0     0  0  0  ...         0         0         0     0      0\n","4        1     1     1  1  1  ...         1         1         1     1      1\n","...    ...   ...   ... .. ..  ...       ...       ...       ...   ...    ...\n","2546     0     0     0  0  0  ...         0         0         0     0      0\n","2547     0     0     0  0  0  ...         0         0         0     0      0\n","2548     0     0     0  0  0  ...         0         0         0     0      0\n","2549     1     1     1  1  1  ...         1         1         1     1      1\n","2550     1     1     1  1  1  ...         1         1         1     1      1\n","\n","[2551 rows x 59 columns]\n"," \n","end of cycle: BbAHh\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbMx<2.5  BbAv<2.5  BbAH  BbAHh  BbMxAHH\n","0        0     0     0  0  0  ...         0         0     0      0        0\n","1        0     1     0  0  0  ...         0         0     0      0        0\n","2        1     0     0  0  0  ...         0         0     0      0        0\n","3        1     1     1  1  1  ...         1         1     1      1        1\n","4        0     0     0  0  0  ...         0         0     0      0        0\n","...    ...   ...   ... .. ..  ...       ...       ...   ...    ...      ...\n","2546     1     0     0  0  0  ...         0         0     0      0        0\n","2547     1     1     1  1  1  ...         1         1     1      1        1\n","2548     1     1     1  1  1  ...         1         1     1      1        1\n","2549     1     1     1  1  1  ...         1         1     1      1        1\n","2550     1     1     1  1  1  ...         1         1     1      1        1\n","\n","[2551 rows x 60 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbMx<2.5  BbAv<2.5  BbAH  BbAHh  BbMxAHH\n","0        1     1     1  1  1  ...         1         1     1      1        1\n","1        1     0     0  0  0  ...         0         0     0      0        0\n","2        0     1     0  0  0  ...         0         0     0      0        0\n","3        0     0     0  0  0  ...         0         0     0      0        0\n","4        0     0     0  0  0  ...         0         0     0      0        0\n","...    ...   ...   ... .. ..  ...       ...       ...   ...    ...      ...\n","2546     0     1     1  1  1  ...         1         1     1      1        1\n","2547     0     0     0  0  0  ...         0         0     0      0        0\n","2548     0     0     0  0  0  ...         0         0     0      0        0\n","2549     0     0     0  0  0  ...         0         0     0      0        0\n","2550     0     0     0  0  0  ...         0         0     0      0        0\n","\n","[2551 rows x 60 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     1.96     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbMx<2.5  BbAv<2.5  BbAH  BbAHh  BbMxAHH\n","0        0     0     0  0  0  ...         0         0     0      0        0\n","1        1     1     1  1  1  ...         1         1     1      1        1\n","2        1     1     1  1  1  ...         1         1     1      1        1\n","3        0     0     0  0  0  ...         0         0     0      0        0\n","4        1     1     1  1  1  ...         1         1     1      1        1\n","...    ...   ...   ... .. ..  ...       ...       ...   ...    ...      ...\n","2546     0     0     0  0  0  ...         0         0     0      0        0\n","2547     0     0     0  0  0  ...         0         0     0      0        0\n","2548     0     0     0  0  0  ...         0         0     0      0        0\n","2549     1     1     1  1  1  ...         1         1     1      1        1\n","2550     1     1     1  1  1  ...         1         1     1      1        1\n","\n","[2551 rows x 60 columns]\n"," \n","end of cycle: BbMxAHH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH\n","0        0     0     0  0  0  ...         0     0      0        0        0\n","1        0     1     0  0  0  ...         0     0      0        0        0\n","2        1     0     0  0  0  ...         0     0      0        0        0\n","3        1     1     1  1  1  ...         1     1      1        1        1\n","4        0     0     0  0  0  ...         0     0      0        0        0\n","...    ...   ...   ... .. ..  ...       ...   ...    ...      ...      ...\n","2546     1     0     0  0  0  ...         0     0      0        0        0\n","2547     1     1     1  1  1  ...         1     1      1        1        1\n","2548     1     1     1  1  1  ...         1     1      1        1        1\n","2549     1     1     1  1  1  ...         1     1      1        1        1\n","2550     1     1     1  1  1  ...         1     1      1        1        1\n","\n","[2551 rows x 61 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH\n","0        1     1     1  1  1  ...         1     1      1        1        1\n","1        1     0     0  0  0  ...         0     0      0        0        0\n","2        0     1     0  0  0  ...         0     0      0        0        0\n","3        0     0     0  0  0  ...         0     0      0        0        0\n","4        0     0     0  0  0  ...         0     0      0        0        0\n","...    ...   ...   ... .. ..  ...       ...   ...    ...      ...      ...\n","2546     0     1     1  1  1  ...         1     1      1        1        1\n","2547     0     0     0  0  0  ...         0     0      0        0        0\n","2548     0     0     0  0  0  ...         0     0      0        0        0\n","2549     0     0     0  0  0  ...         0     0      0        0        0\n","2550     0     0     0  0  0  ...         0     0      0        0        0\n","\n","[2551 rows x 61 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     2.04     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.97     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH\n","0        0     0     0  0  0  ...         0     0      0        0        0\n","1        1     1     1  1  1  ...         1     1      1        1        1\n","2        1     1     1  1  1  ...         1     1      1        1        1\n","3        0     0     0  0  0  ...         0     0      0        0        0\n","4        1     1     1  1  1  ...         1     1      1        1        1\n","...    ...   ...   ... .. ..  ...       ...   ...    ...      ...      ...\n","2546     0     0     0  0  0  ...         0     0      0        0        0\n","2547     0     0     0  0  0  ...         0     0      0        0        0\n","2548     0     0     0  0  0  ...         0     0      0        0        0\n","2549     1     1     1  1  1  ...         1     1      1        1        1\n","2550     1     1     1  1  1  ...         1     1      1        1        1\n","\n","[2551 rows x 61 columns]\n"," \n","end of cycle: BbAvAHH\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     2.01     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     2.00     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     1.81     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     2.10     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     1.86     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     1.89     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     2.06     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     1.90     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     2.08     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.96     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA\n","0        0     0     0  0  0  ...     0      0        0        0        0\n","1        0     1     0  0  0  ...     0      0        0        0        0\n","2        1     0     0  0  0  ...     0      0        0        0        0\n","3        1     1     1  1  1  ...     1      1        1        1        1\n","4        0     0     0  0  0  ...     0      0        0        0        0\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546     1     0     0  0  0  ...     0      0        0        0        0\n","2547     1     1     1  1  1  ...     1      1        1        1        1\n","2548     1     1     1  1  1  ...     1      1        1        1        1\n","2549     1     1     1  1  1  ...     1      1        1        1        1\n","2550     1     1     1  1  1  ...     1      1        1        1        1\n","\n","[2551 rows x 62 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     2.01     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     2.00     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     1.81     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     2.10     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     1.86     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     1.89     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     2.06     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     1.90     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     2.08     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.96     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA\n","0        1     1     1  1  1  ...     1      1        1        1        1\n","1        1     0     0  0  0  ...     0      0        0        0        0\n","2        0     1     0  0  0  ...     0      0        0        0        0\n","3        0     0     0  0  0  ...     0      0        0        0        0\n","4        0     0     0  0  0  ...     0      0        0        0        0\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546     0     1     1  1  1  ...     1      1        1        1        1\n","2547     0     0     0  0  0  ...     0      0        0        0        0\n","2548     0     0     0  0  0  ...     0      0        0        0        0\n","2549     0     0     0  0  0  ...     0      0        0        0        0\n","2550     0     0     0  0  0  ...     0      0        0        0        0\n","\n","[2551 rows x 62 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     2.01     1.84\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     2.00     1.86\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     1.81     2.05\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     2.10     1.76\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     1.86     2.00\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     1.89     1.97\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     2.06     1.81\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     1.90     1.97\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     2.08     1.81\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.96     1.90\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA\n","0        0     0     0  0  0  ...     0      0        0        0        0\n","1        1     1     1  1  1  ...     1      1        1        1        1\n","2        1     1     1  1  1  ...     1      1        1        1        1\n","3        0     0     0  0  0  ...     0      0        0        0        0\n","4        1     1     1  1  1  ...     1      1        1        1        1\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546     0     0     0  0  0  ...     0      0        0        0        0\n","2547     0     0     0  0  0  ...     0      0        0        0        0\n","2548     0     0     0  0  0  ...     0      0        0        0        0\n","2549     1     1     1  1  1  ...     1      1        1        1        1\n","2550     1     1     1  1  1  ...     1      1        1        1        1\n","\n","[2551 rows x 62 columns]\n"," \n","end of cycle: BbMxAHA\n","x9 after droping column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     2.01     1.92\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     2.00     1.93\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     1.81     2.11\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     2.10     1.83\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     1.86     2.04\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     1.89     2.03\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     2.06     1.84\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     1.90     2.02\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     2.08     1.85\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.96     1.97\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1\n"," 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0\n"," 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n"," 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0\n"," 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n"," 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n"," 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n"," 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0\n"," 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1\n"," 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n"," 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1\n"," 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n"," 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n"," 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"," \n","the train prediction looks like this: [0 1 1 ... 1 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0        0     0     0  0  0  ...      0        0        0        0        0\n","1        0     1     0  0  0  ...      0        0        0        0        0\n","2        1     0     0  0  0  ...      0        0        0        0        0\n","3        1     1     1  1  1  ...      1        1        1        1        1\n","4        0     0     0  0  0  ...      0        0        0        0        0\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546     1     0     0  0  0  ...      0        0        0        0        0\n","2547     1     1     1  1  1  ...      1        1        1        1        1\n","2548     1     1     1  1  1  ...      1        1        1        1        1\n","2549     1     1     1  1  1  ...      1        1        1        1        1\n","2550     1     1     1  1  1  ...      1        1        1        1        1\n","\n","[2551 rows x 63 columns]\n"," \n","x19 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     2.01     1.92\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     2.00     1.93\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     1.81     2.11\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     2.10     1.83\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     1.86     2.04\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     1.89     2.03\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     2.06     1.84\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     1.90     2.02\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     2.08     1.85\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.96     1.97\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n"," 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0\n"," 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n"," 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n"," 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n"," 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n"," 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0\n"," 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n"," 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n"," 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n"," 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 0 0]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0        1     1     1  1  1  ...      1        1        1        1        1\n","1        1     0     0  0  0  ...      0        0        0        0        0\n","2        0     1     0  0  0  ...      0        0        0        0        0\n","3        0     0     0  0  0  ...      0        0        0        0        0\n","4        0     0     0  0  0  ...      0        0        0        0        0\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546     0     1     1  1  1  ...      1        1        1        1        1\n","2547     0     0     0  0  0  ...      0        0        0        0        0\n","2548     0     0     0  0  0  ...      0        0        0        0        0\n","2549     0     0     0  0  0  ...      0        0        0        0        0\n","2550     0     0     0  0  0  ...      0        0        0        0        0\n","\n","[2551 rows x 63 columns]\n"," \n","x29 after droping the column is:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA\n","0      2.0   2.0   2.0  0  0  ...  22.0  -0.25     2.10     2.01     1.92\n","1      2.0   0.0   1.0  0  1  ...  23.0   0.75     2.05     2.00     1.93\n","2      0.0   0.0   0.0  0  0  ...  21.0   0.00     1.85     1.81     2.11\n","3      0.0   0.0   0.0  0  0  ...  23.0  -0.75     2.19     2.10     1.83\n","4      0.0   0.0   0.0  0  0  ...  21.0   0.25     1.89     1.86     2.04\n","...    ...   ...   ... .. ..  ...   ...    ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...  20.0   1.25     1.95     1.89     2.03\n","2547   0.0   0.0   0.0  0  0  ...  20.0   0.25     2.13     2.06     1.84\n","2548   0.0   1.0   0.0  1  0  ...  18.0  -1.25     1.93     1.90     2.02\n","2549   3.0   2.0   2.0  0  0  ...  18.0   0.75     2.14     2.08     1.85\n","2550   2.0   2.0   1.0  1  0  ...  22.0  -1.00     2.01     1.96     1.97\n","\n","[2551 rows x 62 columns]\n"," \n","the test prediction looks like this: [0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n"," 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n"," 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n"," 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n"," 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0\n"," 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n"," 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n"," 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1\n"," 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n"," 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1\n"," 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n"," 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n"," 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0]\n"," \n","the train prediction looks like this: [1 0 0 ... 0 1 1]\n"," \n"," Adding test and Train prediction in a empty dataset:\n","       FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0        0     0     0  0  0  ...      0        0        0        0        0\n","1        1     1     1  1  1  ...      1        1        1        1        1\n","2        1     1     1  1  1  ...      1        1        1        1        1\n","3        0     0     0  0  0  ...      0        0        0        0        0\n","4        1     1     1  1  1  ...      1        1        1        1        1\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546     0     0     0  0  0  ...      0        0        0        0        0\n","2547     0     0     0  0  0  ...      0        0        0        0        0\n","2548     0     0     0  0  0  ...      0        0        0        0        0\n","2549     1     1     1  1  1  ...      1        1        1        1        1\n","2550     1     1     1  1  1  ...      1        1        1        1        1\n","\n","[2551 rows x 63 columns]\n"," \n","end of cycle: BbAvAHA\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"keGtZ8iO3Ogy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1594744940438,"user_tz":240,"elapsed":808,"user":{"displayName":"Pavan Patel","photoUrl":"","userId":"00041028778275492936"}},"outputId":"f7845170-8e48-48e2-db86-cc5ee5310d67"},"source":["x"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FTAG</th>\n","      <th>HTHG</th>\n","      <th>HTAG</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>HS</th>\n","      <th>AS</th>\n","      <th>HST</th>\n","      <th>AST</th>\n","      <th>HF</th>\n","      <th>AF</th>\n","      <th>HC</th>\n","      <th>AC</th>\n","      <th>HY</th>\n","      <th>AY</th>\n","      <th>HR</th>\n","      <th>AR</th>\n","      <th>B365H</th>\n","      <th>B365D</th>\n","      <th>B365A</th>\n","      <th>BWH</th>\n","      <th>BWD</th>\n","      <th>BWA</th>\n","      <th>GBH</th>\n","      <th>GBD</th>\n","      <th>GBA</th>\n","      <th>IWH</th>\n","      <th>IWD</th>\n","      <th>IWA</th>\n","      <th>LBH</th>\n","      <th>LBD</th>\n","      <th>LBA</th>\n","      <th>SBH</th>\n","      <th>SBD</th>\n","      <th>SBA</th>\n","      <th>WHH</th>\n","      <th>WHD</th>\n","      <th>WHA</th>\n","      <th>SJH</th>\n","      <th>SJD</th>\n","      <th>SJA</th>\n","      <th>VCH</th>\n","      <th>VCD</th>\n","      <th>VCA</th>\n","      <th>Bb1X2</th>\n","      <th>BbMxH</th>\n","      <th>BbAvH</th>\n","      <th>BbMxD</th>\n","      <th>BbAvD</th>\n","      <th>BbMxA</th>\n","      <th>BbAvA</th>\n","      <th>BbOU</th>\n","      <th>BbMx&gt;2.5</th>\n","      <th>BbAv&gt;2.5</th>\n","      <th>BbMx&lt;2.5</th>\n","      <th>BbAv&lt;2.5</th>\n","      <th>BbAH</th>\n","      <th>BbAHh</th>\n","      <th>BbMxAHH</th>\n","      <th>BbAvAHH</th>\n","      <th>BbMxAHA</th>\n","      <th>BbAvAHA</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>14.0</td>\n","      <td>16.0</td>\n","      <td>7.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.30</td>\n","      <td>3.25</td>\n","      <td>3.00</td>\n","      <td>2.10</td>\n","      <td>3.25</td>\n","      <td>3.15</td>\n","      <td>2.25</td>\n","      <td>3.20</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>3.0</td>\n","      <td>3.10</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.200</td>\n","      <td>3.20</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.40</td>\n","      <td>2.20</td>\n","      <td>3.25</td>\n","      <td>3.10</td>\n","      <td>56.0</td>\n","      <td>2.40</td>\n","      <td>2.20</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.40</td>\n","      <td>3.05</td>\n","      <td>36.0</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>1.87</td>\n","      <td>1.70</td>\n","      <td>22.0</td>\n","      <td>-0.25</td>\n","      <td>2.10</td>\n","      <td>2.01</td>\n","      <td>1.92</td>\n","      <td>1.84</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>10.0</td>\n","      <td>12.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>15.0</td>\n","      <td>14.0</td>\n","      <td>8.0</td>\n","      <td>6.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>3.40</td>\n","      <td>1.72</td>\n","      <td>4.35</td>\n","      <td>3.35</td>\n","      <td>1.75</td>\n","      <td>4.25</td>\n","      <td>3.40</td>\n","      <td>1.83</td>\n","      <td>3.80</td>\n","      <td>3.1</td>\n","      <td>1.80</td>\n","      <td>3.75</td>\n","      <td>3.20</td>\n","      <td>1.83</td>\n","      <td>4.33</td>\n","      <td>3.600</td>\n","      <td>1.75</td>\n","      <td>4.33</td>\n","      <td>3.20</td>\n","      <td>1.72</td>\n","      <td>4.00</td>\n","      <td>3.25</td>\n","      <td>1.83</td>\n","      <td>4.50</td>\n","      <td>3.30</td>\n","      <td>1.80</td>\n","      <td>56.0</td>\n","      <td>5.65</td>\n","      <td>4.69</td>\n","      <td>3.70</td>\n","      <td>3.36</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.79</td>\n","      <td>23.0</td>\n","      <td>0.75</td>\n","      <td>2.05</td>\n","      <td>2.00</td>\n","      <td>1.93</td>\n","      <td>1.86</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>4.0</td>\n","      <td>12.0</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.37</td>\n","      <td>3.25</td>\n","      <td>2.87</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.80</td>\n","      <td>2.30</td>\n","      <td>3.30</td>\n","      <td>2.95</td>\n","      <td>2.20</td>\n","      <td>3.0</td>\n","      <td>2.90</td>\n","      <td>2.25</td>\n","      <td>3.00</td>\n","      <td>2.88</td>\n","      <td>2.30</td>\n","      <td>3.200</td>\n","      <td>3.00</td>\n","      <td>2.30</td>\n","      <td>3.20</td>\n","      <td>2.62</td>\n","      <td>2.20</td>\n","      <td>3.20</td>\n","      <td>3.00</td>\n","      <td>2.35</td>\n","      <td>3.25</td>\n","      <td>2.80</td>\n","      <td>56.0</td>\n","      <td>2.60</td>\n","      <td>2.31</td>\n","      <td>3.30</td>\n","      <td>3.16</td>\n","      <td>3.05</td>\n","      <td>2.87</td>\n","      <td>36.0</td>\n","      <td>2.24</td>\n","      <td>2.04</td>\n","      <td>1.77</td>\n","      <td>1.69</td>\n","      <td>21.0</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","      <td>2.11</td>\n","      <td>2.05</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","      <td>13.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>11.0</td>\n","      <td>3.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.72</td>\n","      <td>3.40</td>\n","      <td>5.00</td>\n","      <td>1.65</td>\n","      <td>3.45</td>\n","      <td>4.80</td>\n","      <td>1.73</td>\n","      <td>3.45</td>\n","      <td>4.75</td>\n","      <td>1.70</td>\n","      <td>3.2</td>\n","      <td>4.20</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>4.50</td>\n","      <td>1.70</td>\n","      <td>3.400</td>\n","      <td>5.00</td>\n","      <td>1.70</td>\n","      <td>3.30</td>\n","      <td>4.33</td>\n","      <td>1.67</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>1.75</td>\n","      <td>3.25</td>\n","      <td>5.00</td>\n","      <td>55.0</td>\n","      <td>1.80</td>\n","      <td>1.69</td>\n","      <td>3.63</td>\n","      <td>3.38</td>\n","      <td>5.60</td>\n","      <td>4.79</td>\n","      <td>36.0</td>\n","      <td>2.10</td>\n","      <td>1.94</td>\n","      <td>1.90</td>\n","      <td>1.77</td>\n","      <td>23.0</td>\n","      <td>-0.75</td>\n","      <td>2.19</td>\n","      <td>2.10</td>\n","      <td>1.83</td>\n","      <td>1.76</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4.0</td>\n","      <td>16.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.87</td>\n","      <td>3.20</td>\n","      <td>2.40</td>\n","      <td>2.90</td>\n","      <td>3.35</td>\n","      <td>2.20</td>\n","      <td>2.70</td>\n","      <td>3.30</td>\n","      <td>2.45</td>\n","      <td>2.50</td>\n","      <td>3.0</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.70</td>\n","      <td>3.200</td>\n","      <td>2.50</td>\n","      <td>2.75</td>\n","      <td>3.10</td>\n","      <td>2.30</td>\n","      <td>2.75</td>\n","      <td>3.20</td>\n","      <td>2.38</td>\n","      <td>2.80</td>\n","      <td>3.25</td>\n","      <td>2.35</td>\n","      <td>56.0</td>\n","      <td>3.30</td>\n","      <td>2.81</td>\n","      <td>3.35</td>\n","      <td>3.17</td>\n","      <td>2.50</td>\n","      <td>2.35</td>\n","      <td>36.0</td>\n","      <td>2.23</td>\n","      <td>2.02</td>\n","      <td>1.80</td>\n","      <td>1.71</td>\n","      <td>21.0</td>\n","      <td>0.25</td>\n","      <td>1.89</td>\n","      <td>1.86</td>\n","      <td>2.04</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2546</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6.0</td>\n","      <td>17.0</td>\n","      <td>5.0</td>\n","      <td>9.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.44</td>\n","      <td>6.75</td>\n","      <td>4.60</td>\n","      <td>1.42</td>\n","      <td>7.00</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>6.00</td>\n","      <td>4.0</td>\n","      <td>1.47</td>\n","      <td>7.50</td>\n","      <td>4.50</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.600</td>\n","      <td>1.40</td>\n","      <td>7.00</td>\n","      <td>4.33</td>\n","      <td>1.44</td>\n","      <td>7.50</td>\n","      <td>4.80</td>\n","      <td>1.40</td>\n","      <td>8.00</td>\n","      <td>4.80</td>\n","      <td>1.44</td>\n","      <td>39.0</td>\n","      <td>8.30</td>\n","      <td>7.23</td>\n","      <td>4.90</td>\n","      <td>4.51</td>\n","      <td>1.47</td>\n","      <td>1.43</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.60</td>\n","      <td>2.43</td>\n","      <td>2.27</td>\n","      <td>20.0</td>\n","      <td>1.25</td>\n","      <td>1.95</td>\n","      <td>1.89</td>\n","      <td>2.03</td>\n","      <td>1.97</td>\n","    </tr>\n","    <tr>\n","      <th>2547</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>13.0</td>\n","      <td>14.0</td>\n","      <td>8.0</td>\n","      <td>9.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.40</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.30</td>\n","      <td>3.60</td>\n","      <td>2.05</td>\n","      <td>3.25</td>\n","      <td>3.30</td>\n","      <td>2.10</td>\n","      <td>3.00</td>\n","      <td>3.2</td>\n","      <td>2.20</td>\n","      <td>3.40</td>\n","      <td>3.40</td>\n","      <td>2.00</td>\n","      <td>3.25</td>\n","      <td>3.500</td>\n","      <td>2.10</td>\n","      <td>3.40</td>\n","      <td>3.30</td>\n","      <td>2.15</td>\n","      <td>3.25</td>\n","      <td>3.50</td>\n","      <td>2.10</td>\n","      <td>3.60</td>\n","      <td>3.60</td>\n","      <td>2.10</td>\n","      <td>39.0</td>\n","      <td>3.66</td>\n","      <td>3.38</td>\n","      <td>3.72</td>\n","      <td>3.45</td>\n","      <td>2.20</td>\n","      <td>2.10</td>\n","      <td>33.0</td>\n","      <td>1.77</td>\n","      <td>1.71</td>\n","      <td>2.24</td>\n","      <td>2.11</td>\n","      <td>20.0</td>\n","      <td>0.25</td>\n","      <td>2.13</td>\n","      <td>2.06</td>\n","      <td>1.84</td>\n","      <td>1.81</td>\n","    </tr>\n","    <tr>\n","      <th>2548</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>15.0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>7.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>8.50</td>\n","      <td>1.40</td>\n","      <td>4.40</td>\n","      <td>7.75</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.00</td>\n","      <td>1.37</td>\n","      <td>4.4</td>\n","      <td>7.30</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>4.333</td>\n","      <td>8.00</td>\n","      <td>1.40</td>\n","      <td>4.20</td>\n","      <td>9.00</td>\n","      <td>1.40</td>\n","      <td>4.50</td>\n","      <td>7.50</td>\n","      <td>1.40</td>\n","      <td>5.00</td>\n","      <td>9.00</td>\n","      <td>39.0</td>\n","      <td>1.41</td>\n","      <td>1.40</td>\n","      <td>5.07</td>\n","      <td>4.55</td>\n","      <td>9.10</td>\n","      <td>8.04</td>\n","      <td>32.0</td>\n","      <td>1.68</td>\n","      <td>1.63</td>\n","      <td>2.45</td>\n","      <td>2.23</td>\n","      <td>18.0</td>\n","      <td>-1.25</td>\n","      <td>1.93</td>\n","      <td>1.90</td>\n","      <td>2.02</td>\n","      <td>1.97</td>\n","    </tr>\n","    <tr>\n","      <th>2549</th>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>12.0</td>\n","      <td>12.0</td>\n","      <td>8.0</td>\n","      <td>8.0</td>\n","      <td>12.0</td>\n","      <td>10.0</td>\n","      <td>9.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.62</td>\n","      <td>5.00</td>\n","      <td>3.75</td>\n","      <td>1.65</td>\n","      <td>5.40</td>\n","      <td>3.6</td>\n","      <td>1.57</td>\n","      <td>4.50</td>\n","      <td>3.75</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>3.750</td>\n","      <td>1.65</td>\n","      <td>5.00</td>\n","      <td>3.60</td>\n","      <td>1.70</td>\n","      <td>5.00</td>\n","      <td>4.00</td>\n","      <td>1.67</td>\n","      <td>5.25</td>\n","      <td>4.20</td>\n","      <td>1.67</td>\n","      <td>39.0</td>\n","      <td>5.60</td>\n","      <td>5.02</td>\n","      <td>4.28</td>\n","      <td>3.93</td>\n","      <td>1.70</td>\n","      <td>1.65</td>\n","      <td>29.0</td>\n","      <td>1.67</td>\n","      <td>1.58</td>\n","      <td>2.50</td>\n","      <td>2.31</td>\n","      <td>18.0</td>\n","      <td>0.75</td>\n","      <td>2.14</td>\n","      <td>2.08</td>\n","      <td>1.85</td>\n","      <td>1.81</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>14.0</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>7.0</td>\n","      <td>15.0</td>\n","      <td>9.0</td>\n","      <td>9.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>1.60</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>5.75</td>\n","      <td>1.60</td>\n","      <td>3.6</td>\n","      <td>5.00</td>\n","      <td>1.57</td>\n","      <td>3.75</td>\n","      <td>6.00</td>\n","      <td>1.55</td>\n","      <td>4.000</td>\n","      <td>5.60</td>\n","      <td>1.62</td>\n","      <td>3.60</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>4.00</td>\n","      <td>6.00</td>\n","      <td>1.57</td>\n","      <td>4.30</td>\n","      <td>6.00</td>\n","      <td>39.0</td>\n","      <td>1.62</td>\n","      <td>1.57</td>\n","      <td>4.35</td>\n","      <td>3.95</td>\n","      <td>6.40</td>\n","      <td>5.80</td>\n","      <td>28.0</td>\n","      <td>1.67</td>\n","      <td>1.59</td>\n","      <td>2.43</td>\n","      <td>2.26</td>\n","      <td>22.0</td>\n","      <td>-1.00</td>\n","      <td>2.01</td>\n","      <td>1.96</td>\n","      <td>1.97</td>\n","      <td>1.90</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2551 rows × 63 columns</p>\n","</div>"],"text/plain":["      FTAG  HTHG  HTAG  D  E  ...  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA\n","0      2.0   2.0   2.0  0  0  ...  -0.25     2.10     2.01     1.92     1.84\n","1      2.0   0.0   1.0  0  1  ...   0.75     2.05     2.00     1.93     1.86\n","2      0.0   0.0   0.0  0  0  ...   0.00     1.85     1.81     2.11     2.05\n","3      0.0   0.0   0.0  0  0  ...  -0.75     2.19     2.10     1.83     1.76\n","4      0.0   0.0   0.0  0  0  ...   0.25     1.89     1.86     2.04     2.00\n","...    ...   ...   ... .. ..  ...    ...      ...      ...      ...      ...\n","2546   1.0   0.0   1.0  0  1  ...   1.25     1.95     1.89     2.03     1.97\n","2547   0.0   0.0   0.0  0  0  ...   0.25     2.13     2.06     1.84     1.81\n","2548   0.0   1.0   0.0  1  0  ...  -1.25     1.93     1.90     2.02     1.97\n","2549   3.0   2.0   2.0  0  0  ...   0.75     2.14     2.08     1.85     1.81\n","2550   2.0   2.0   1.0  1  0  ...  -1.00     2.01     1.96     1.97     1.90\n","\n","[2551 rows x 63 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"XBGTSHIwJmNK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}